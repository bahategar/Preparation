{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770b64d6-bde3-4586-8e50-240c60fcaabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "from torchinfo import summary\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39c1c1-0d34-4c29-bdf8-b6cb5a0bdd66",
   "metadata": {},
   "source": [
    "![flow-model-requirement](images/Flow-min-requirement-Training-FineTuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb819b-e0ef-4e47-920a-13220432429b",
   "metadata": {},
   "source": [
    "# Tokenizer Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745520b1-db9f-4e28-a18d-dcb8971ecf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = 'bert-base-uncased'\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d2526cf-ead8-44c5-97b7-bf21c11b6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for tokenizing the dataset\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572420f8-b374-408d-bd76-d2f8589dc0c0",
   "metadata": {},
   "source": [
    "# Load Dataset and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a950c03-eabd-46cb-b461-cb596ed99216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/amazon_polarity\n",
    "# takes a long time to process, you may want to try it yourself\n",
    "# dataset = load_dataset('amazon_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c9d05a-eb40-4949-b656-7cb8dd56ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('glue', 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d1f963d-d0c6-4ba0-8f7b-23849b10d12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "618d3181-32a1-419b-bf58-e60200b39c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['hide new secretions from the parental units ',\n",
       "  'contains no wit , only labored gags ',\n",
       "  'that loves its characters and communicates something rather beautiful about human nature '],\n",
       " 'label': [0, 0, 1],\n",
       " 'idx': [0, 1, 2]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de940c8a-a0a3-4513-8a09-e3c2d83cba3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a834e3176e064bafbf58187f08f27b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "013443d6-dfd9-451d-8eb0-1f97a7be7197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6694d4d2-13ad-4c30-ab00-40154adb3f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['hide new secretions from the parental units ',\n",
       "  'contains no wit , only labored gags ',\n",
       "  'that loves its characters and communicates something rather beautiful about human nature '],\n",
       " 'label': [0, 0, 1],\n",
       " 'idx': [0, 1, 2],\n",
       " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
       "  [101, 3397, 2053, 15966, 1010, 2069, 4450, 2098, 18201, 2015, 102],\n",
       "  [101,\n",
       "   2008,\n",
       "   7459,\n",
       "   2049,\n",
       "   3494,\n",
       "   1998,\n",
       "   10639,\n",
       "   2015,\n",
       "   2242,\n",
       "   2738,\n",
       "   3376,\n",
       "   2055,\n",
       "   2529,\n",
       "   3267,\n",
       "   102]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b8abf-97b4-4524-ae5a-41ed9b3ba9ea",
   "metadata": {},
   "source": [
    "# Training Arguments Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ff5692b-acc1-41a5-975f-cde965141e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='trainer_demo',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    "    report_to='none',\n",
    "    fp16=True,  # Enable mixed-precision training if your GPU supports it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd078e48-1275-46a2-a4bb-3b442cc32e6c",
   "metadata": {},
   "source": [
    "# Main Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22968d85-765d-452d-aa5f-132d45480c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9afa1e1-3b3d-4cbf-97d6-4aa13d1526bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1c497-ae50-4dc4-a0d5-92e1566c4227",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d4418dc-ef1c-4514-a648-7dbd83d2c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True if a compatible GPU is detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db5ecfd9-c24b-47e1-b316-90d74c01e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# After defining the trainer, before starting the training\n",
    "print(\"Model device:\", next(model.parameters()).device)  # Should output \"cuda\" if on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71332d3f-6f65-4ee2-b7ad-f9a1b8ed03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "params_before = []\n",
    "for name, p in model.named_parameters():\n",
    "    params_before.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e8b5db5-3de7-4ef0-9ed6-5687daec0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics function\n",
    "# It helps to evaluate performance model per epochs.\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc3eb519-a087-4a86-8f5d-4ce9cfa3465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Data Collator:  DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "# Define Trainer Object\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Default Data Collator: \", trainer.data_collator)\n",
    "# NOTE: By default data_collator use DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf458895-bfff-41d9-87f7-1e44082c9fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8419/8419 09:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.413721</td>\n",
       "      <td>0.893349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.1468258087560561, metrics={'train_runtime': 545.6289, 'train_samples_per_second': 123.434, 'train_steps_per_second': 15.43, 'total_flos': 517212489917652.0, 'train_loss': 0.1468258087560561, 'epoch': 1.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33404e6d-3af2-495a-bdac-cb928e84eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually evaluation model\n",
    "# metric = evaluate.load('accuracy')\n",
    "\n",
    "# predictions = trainer.predict(tokenized_datasets['validation'])\n",
    "# logits, labels = predictions.predictions, predictions.label_ids\n",
    "\n",
    "# accuracy = metric.compute(predictions=np.argmax(logits, axis=-1), references=labels)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154a16e-e9dd-40df-9db0-96b2f979783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# trainer.save_model('model_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d7fc2-cba1-4811-82de-a9f22f43780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do sanity Check\n",
    "params_after = []\n",
    "for name, p in model.named_parameters():\n",
    "  params_after.append(p.detach().cpu().numpy())\n",
    "\n",
    "for p1, p2 in zip(params_before, params_after):\n",
    "  print(np.sum(np.abs(p1 - p2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1468b03-76af-4520-a4ee-b21f8332be00",
   "metadata": {},
   "source": [
    "# Fix Naming Label Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd62123-23e6-48f5-9587-adf4b5c87243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = './model_demo/config.json'\n",
    "# with open(config_path) as f:\n",
    "#     j = json.load(f)\n",
    "#     print(\"Before updated:\")\n",
    "#     print(j)\n",
    "\n",
    "#     # Add 'id2label' for determine the label name\n",
    "#     j['id2label'] = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "#     with open(config_path, 'w') as f:\n",
    "#         json.dump(j, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b13cd6-1a8a-4679-8f9a-e314a637c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross check\n",
    "# with open(config_path) as f:\n",
    "#     j = json.load(f)\n",
    "\n",
    "# print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58b80d-3090-4fc9-812d-8a522622ee3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
