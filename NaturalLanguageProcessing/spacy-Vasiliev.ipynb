{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef06b42-16d0-48fe-8680-b554d2eacf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed7faa5-fa4c-4ed1-96d0-68308b37ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NLP object (load the model that installed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cdb3ee-98d3-4105-b004-3e85851155f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72e4e7-c459-4b3d-b1aa-cc7cd4019e3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1 - Introduction\n",
    "\n",
    "### What is Natural Language Processing\n",
    "- Natural Language Processing is a subfield of artificial intelligence that tries to **process and analyze natural language data.**\n",
    "  \n",
    "NOTE: Natural language is a language that developed naturally through use.\n",
    "\n",
    "The idea: Since we already know semantic and grammar rules of human language, then we can build applications that can progammatically understand utterances in that language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd11414-e853-4b18-9d08-c30e02b25b87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How can Computers Understand Language\n",
    "- Since computer (or machine) only understand number, we need to convert language words into numbers. This process called **Word Embedding**.\n",
    "- Word Embedding concept: **Mapping the words to vectors of real numbers that distribute the meaning of each word** between the coordinates of the corresponding word vector. NOTE:\n",
    "    - Words that similar (in machine perspectives) if their word vector are nearby.\n",
    "    - Two words are distributed nearby in the vector space based on **the contextual similarity** of their usage in a large corpus of text.\n",
    "      \n",
    "      NOTE: Key factors that influence are (1) Co-occurence in Similar Contexts and (2) Frequency of Co-occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66d93b-826e-4260-8160-4597d6c95088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dependency Grammars vs Phrase Structure Grammars\n",
    "> spaCy primarily uses dependency grammar for syntactic parsing. It builds dependency trees that represent the relationships between words in a sentence, showing how each word depends on another.\n",
    "\n",
    "**Phrase Structure Grammars**\n",
    "- Based on how words combine to form constituents in a sentence ==> Focus on **Relation between constituents**\n",
    "  \n",
    "    NOTE: Constituents is a group of words that functions as a single unit in a sentence. E.g: Morphem, Frasa, and Clause.\n",
    "\n",
    "- Concept: The rule decompose a sentence into its constituent parts until reach one unit word in a hierarchical way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ec731-2fc2-4b50-9b8f-53806c9ec00a",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "![Images](data_spacy/images/ex-parse-structure-grammar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b6413-8002-4476-b67b-ff9b507b697d",
   "metadata": {},
   "source": [
    "**Dependency Structure Grammars**\n",
    "- Based on the **relations between individual words.**\n",
    "- Concept: Determine root which is a main word of sentence (usually verb, we called it main verb). Then all other words are directly or indirectly dependent on this root.\n",
    "    NOTE:\n",
    "      (1) We call the independent word as \"HEAD\" and the dependent word as \"CHILD\".\n",
    "      (2) Each word in a sentence must be connected to exactly one HEAD (ROOT is connected to itself). The same word might have None, one, or several CHILDREN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759d2e8-0d6b-4c7e-b40f-ef8389133a02",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "![Images](data_spacy/images/ex-dep-structure-grammar.png)\n",
    "\n",
    "Explanation:\n",
    "- \"sat\" is ROOT\n",
    "- \"sat\" is HEAD of \"cat\", \"on\", and \"mat\"\n",
    "- \"cat\" is HEAD of \"the\"\n",
    "- \"mat\" is HEAD of \"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a3d48-dc8e-440d-b760-941bfde185b3",
   "metadata": {},
   "source": [
    "### Common Grammar Concept\n",
    "\n",
    "**Transitive Verbs and Direct Objects**\n",
    "- A directly object is a noun (or a noun phrase) that **directly receives the action** of the verb.\n",
    "- A transitive verb is an **action verb that needs something (or someone) to receive the action**. This \"something\" that receive the action is **direct object**.\n",
    "\n",
    "For example:\n",
    "\n",
    "    \"She wrote a letter\" ==> Direct object is \"a letter\" and Transitive verb is \"wrote\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d375ab-f535-4ef8-a9c3-9adcc5dd4a9d",
   "metadata": {},
   "source": [
    "**Prepositional Objects**\n",
    "- A preposition **connects noun phrases (or noun, pronoun) with other words in a sentence**.\n",
    "    Example: \"in\", \"above\", \"at\", \"to\", \"of\", etc.\n",
    "- Object of a Preprosition: A noun (or pronoun, noun phrase) that follows a preposition.\n",
    "\n",
    "Example:\n",
    "\n",
    "    \"I wrote a series of articles.\" ==> Preposition is \"of\" and Object of preposition is \"articles\".\n",
    "\n",
    "NOTE:\n",
    "In some questions, extracting the object of preposition might give us the most informative word or phrase in terms finding the answer.\n",
    "\n",
    "Example:\n",
    "\n",
    "    \"What can be done about climate change?\" ==> The phrase “climate change” is the key phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bd5be-f1b1-4487-8043-064f154d015b",
   "metadata": {},
   "source": [
    "**Modal Auxiliary Verbs**\n",
    "- Modal auxiliary verbs are **special verbs used alongside a main verb** to express various moods or modalities. Example: \"may\", \"might\", \"can\", etc.\n",
    "\n",
    "    NOTE:\n",
    "  \n",
    "        Modal auxiliary verbs are special verbs used alongside a main verb to express various moods or modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6c1cc-1d50-4142-bee5-dd922fb57b3c",
   "metadata": {},
   "source": [
    "**Personal Pronoun**\n",
    "- A personal pronoun refers to a specific person, object, or to multiple people or objects.\n",
    "\n",
    "Forms according to their grammatical role in a sentence:\n",
    "- The nominative form (I, you, he, she, it, we, they) is typically used as the nominal subject of a verb.\n",
    "- The accusative form (me, you, him, her, it, us, them) is typically used as the object of a verb or preposition.\n",
    "- The reflexive form (myself, yourself/yourselves, himself, herself, itself, ourselves, themselves) typically refers back to the subject specified within the same clause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbcc3e-95ae-42b7-bb50-9cf4f1674b4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 - Basic NLP Operations with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed24158-4a46-470c-98ed-d22d46c9d83c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "> Parsing the text input into tokens.\n",
    "\n",
    "- By default, SpaCy tokenize text into word-level tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8de33cb-6ace-4c48-ae17-c6b31000c6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I\n",
      "2. am\n",
      "3. flying\n",
      "4. to\n",
      "5. Frisco\n"
     ]
    }
   ],
   "source": [
    "text = \"I am flying to Frisco\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for idx, token in enumerate(doc):\n",
    "    print(f\"{idx + 1}. {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bac49-be2b-4525-82cb-7ec6994f0c80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lemmatization\n",
    "> Process of reducing word forms to their lemma.\n",
    "\n",
    "NOTE: Lemma is a base form of a token. For example \"flying\" is \"fly\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfa3c5a-e568-4140-a7df-b9cb24145f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Lemma\n",
      "This this\n",
      "product product\n",
      "integrates integrate\n",
      "both both\n",
      "libraries library\n",
      "for for\n",
      "downloading download\n",
      "and and\n",
      "applying apply\n",
      "patches patch\n"
     ]
    }
   ],
   "source": [
    "text = \"This product integrates both libraries for downloading and applying patches\"\n",
    "doc = nlp(text)\n",
    "print(\"Text\", \"Lemma\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0ed95-a875-4075-97bc-4b245c8669a1",
   "metadata": {},
   "source": [
    "**Case Application Lemmatization: Meaning Recognition.**\n",
    "\n",
    "Suppose that we have NLP application that interacting with an online system that provides an API for booking tickets for trips.\n",
    "\n",
    "The application processes a customer's request, extracting necessary information from it, and then passing on that information to the underlying API.\n",
    "\n",
    "The two information that needed to extract:\n",
    "1. Determine whether the customer wants an air ticket, a railway ticket, or a bus ticket.\n",
    "2. Extract its destination.\n",
    "\n",
    "Ideas:\n",
    "1. Tokenization\n",
    "2. Lemmatization\n",
    "3. Match each token into a predefined list of keywords that help for mapping token into word that represent the customer needed.\n",
    "\n",
    "NOTE: Lemmatization helps programmer to create simple predifined list of keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecdcf7-dc4c-4ac3-bc8c-c297d673c375",
   "metadata": {},
   "source": [
    "![Images](data_spacy/images/ex-app-lemma.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16c005f-3618-4fb6-aac2-c7714c9b05de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Lemma\n",
      "I I\n",
      "am be\n",
      "flying fly\n",
      "to to\n",
      "Frisco San Francisco\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH, NORM\n",
    "from spacy.language import Language\n",
    "\n",
    "# Example simple program\n",
    "\n",
    "input_text = \"I am flying to Frisco\"\n",
    "\n",
    "# Modeling\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add special case rules (for special token)\n",
    "special_case = [{ORTH: \"Frisco\", NORM: \"San Francisco\"}]\n",
    "nlp.tokenizer.add_special_case(\"Frisco\", special_case)\n",
    "\n",
    "# Custom pipeline component to adjust lemmas\n",
    "@Language.component(\"custom_lemma_setter\")\n",
    "def custom_lemma_setter(doc):\n",
    "    for token in doc:\n",
    "        if token.text == \"Frisco\":\n",
    "            token.lemma_ = \"San Francisco\"  # Set custom lemma\n",
    "    return doc\n",
    "\n",
    "# Add the component after the default lemmatizer in the pipeline\n",
    "nlp.add_pipe(\"custom_lemma_setter\", after=\"lemmatizer\")\n",
    "\n",
    "doc = nlp(input_text)\n",
    "\n",
    "print(\"Text\", \"Lemma\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a797bce-c134-4d87-ae8d-9a82767dd54b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Part-of-Speech Tagging\n",
    "> Telling part-of-speech of a given word in a given sentence (noun, verb, and so on).\n",
    "\n",
    "In SpaCy, there are two types parts of speech:\n",
    "1. Coarse-gained parts of speech\n",
    "> Using Token.pos (int) and Token.pos_ (unicode) attributes. \n",
    "\n",
    "2. Fine-grained parts of speech\n",
    "> Using Token.tag (int) and Token.tag_ (unicode) attributes.\n",
    "\n",
    "NOTE: In English, the core of parts of speech include noun, pronoun, determiner, adjective, verb, adverb, preposition, conjunction, and injection.\n",
    "\n",
    "\n",
    "Detail about pos tagging: https://v2.spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0ee653-efd7-480d-a708-dc0519c1bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The DET DT\n",
      "2. United PROPN NNP\n",
      "3. States PROPN NNP\n",
      "4. is AUX VBZ\n",
      "5. a DET DT\n",
      "6. country NOUN NN\n",
      "7. primarily ADV RB\n",
      "8. located VERB VBN\n",
      "9. in ADP IN\n",
      "10. North PROPN NNP\n",
      "11. America PROPN NNP\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "text = \"The United States is a country primarily located in North America\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for idx, token in enumerate(doc):\n",
    "    print(f\"{idx + 1}. {token.text}\", token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ead5d-6be5-4741-bb4e-5456157e7c92",
   "metadata": {},
   "source": [
    "**Case Application POS Tags: Find Relevant Verbs.**\n",
    "\n",
    "Continue our case ticket online system NLP.Since there are possible way to express a word (for example in past, present, future), we need to filter it into what we expect. For example:\n",
    "\n",
    "- I flew to LA.\n",
    "- I have flown to LA.\n",
    "- I need to fly to LA.\n",
    "- I am flying to LA.\n",
    "- I will fly to LA.\n",
    "\n",
    "Notice that although all of these sentences would include the “fly to LA” combination if reduced to lemmas, only some of them imply the customer’s intent to book a plane ticket to LA. The first two definitely aren’t suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799242cf-a7e3-445b-9a7c-a39032e58087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Lemma\n",
      "flying -> fly\n",
      "Frisco -> San Francisco\n"
     ]
    }
   ],
   "source": [
    "# According to the table, we can select token with tag_ into 'VBG' or 'VB'\n",
    "# The location will expected to recognize as PROPN\n",
    "\n",
    "input_text = \"I have flown to LA. Now I am flying to Frisco.\"\n",
    "doc = nlp(input_text)\n",
    "print(\"Text\", \"Lemma\")\n",
    "current_idx = 0\n",
    "for token in doc:\n",
    "    if token.tag_ == \"VBG\" or token.tag_ == 'VB':\n",
    "        print(token.text, '->', token.lemma_)\n",
    "        current_idx = token.i\n",
    "    if token.pos_ == \"PROPN\" and current_idx < token.i and current_idx != 0:\n",
    "        print(token.text, '->', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f438c-14a5-497c-9f40-b7c735b2d593",
   "metadata": {},
   "source": [
    "NOTE: We need to improve our model by adding context of the input text.\n",
    "\n",
    "For example “I’m already in the sky, flying to LA.” or “I’m going to fly to LA.” When submitted to the ticket booking NLP application, the application should interpret only one of these sentences as “I need an air ticket to LA.” "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e02f8a-0f2b-4cc5-b357-93ac8875545b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Syntactic Relations: Dependency Parser\n",
    "\n",
    "> Dependency parser helps discover syntactic relations between individual tokens in a sentence and connects syntactically related pairs of words with a single arc.\n",
    "\n",
    "\n",
    "Concept:\n",
    "- Head and Child ==> Since it describes syntactic relation between two words, then one word is called Head (or Parent) and the other is child (or Dependent).\n",
    "  \n",
    "      NOTE:\n",
    "  - Each word in a sentence has exactly one head. Consequently, a word can be a child only to one head.\n",
    "  - If a token head is itself, then it is labeled as ROOT.\n",
    "  - A dependency label is always assigned to the child (the arrow in graphical representation always start from head to child).\n",
    " \n",
    "NOTE:\n",
    "- Every complete sentence should have a **verb** with the **ROOT tag** and a subject with the **nsubj tag**. The oterh elements are optional. \n",
    "  \n",
    "\n",
    "Detail info about Dependency: https://v2.spacy.io/api/annotation#dependency-parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c67deeb-33f6-4e94-84c4-4453b7b175b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Dependency Head\n",
      "I nsubj need\n",
      "need ROOT need\n",
      "a det ticket\n",
      "plane compound ticket\n",
      "ticket dobj need\n"
     ]
    }
   ],
   "source": [
    "# NOTES:\n",
    "# using Token.head to access head token object.\n",
    "# using Token.dep_ to access its dependency\n",
    "\n",
    "# Example\n",
    "text = \"I need a plane ticket\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Token\", \"Dependency\", \"Head\")\n",
    "for token in doc:\n",
    "    print(token, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c758ddd-f7b0-4618-9938-470067c70747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"13ccc8d60d4d43548ee29b0dc84152bc-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">need</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">plane</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">ticket</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-13ccc8d60d4d43548ee29b0dc84152bc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize it\n",
    "from spacy import displacy\n",
    "# Style as dependency\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586fd676-9ab7-4902-afee-52504e36d71d",
   "metadata": {},
   "source": [
    "**Case Application Syntactic Relations: Find Relevant Verbs.**\n",
    "\n",
    "Continue our case ticket online system NLP. Since there are possible the text input is conjugation of two sentences or more, we need to improve our filter. For example:\n",
    "\n",
    "\"I have flown to LA. Now I am flying to Frisco\"\n",
    "\n",
    "NOTE: In this case, we need a ticket into San Fransisco instead LA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27928d4-aa3a-4562-b91a-0fb4dd76a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flown, LA]\n",
      "[flying, Frisco]\n"
     ]
    }
   ],
   "source": [
    "# Concept: From the pattern of text,\n",
    "#  The verb that represent what transportation do customer needed and the Location as preposition object.\n",
    "\n",
    "text = \"I have flown to LA. Now I am flying to Frisco\"\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "# Tokenize into sentence level\n",
    "sentences = list(doc.sents)\n",
    "for sent in sentences:\n",
    "    storage = []\n",
    "    for word in sent:\n",
    "        if word.dep_ == 'ROOT' or word.dep_ == 'pobj':\n",
    "            storage.append(word)\n",
    "    print(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30d5f6-acd2-4932-bebb-fcf70e2f5d83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Named Entity Recognition\n",
    "> A named entity is a real object that you can refer to by a proper name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83857677-68a2-4e95-98cf-3895eba9a5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Entity\n",
      "LA GPE\n",
      "Frisco PERSON\n"
     ]
    }
   ],
   "source": [
    "# NOTES:\n",
    "# - Using Token.ent_type_ to return Entity type of token\n",
    "# - Using Tojen.ent_type to return its int format\n",
    "\n",
    "text = \"I have flown to LA. Now I am flying to Frisco\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Token\", \"Entity\")\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046534ae-cf39-4c38-b7db-2417cadba7f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3 - Working with Container Objects and Customizing Spacy\n",
    "\n",
    "### Container Objects\n",
    "\n",
    "- The main objects of SpaCy:\n",
    "    1. Container Object: Object that grouping multiple element into a single unit. It can be collected of objects (like tokens or sentences) or a set of annotation. \n",
    "    2. Pipeline Components (such as part-of-speech tagger and named entity recognizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd987d4f-3fc4-4828-8568-e1bb3f3532e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit way:  Hi there \n",
      "Return from Spacy NLP Model:  Hi there\n"
     ]
    }
   ],
   "source": [
    "# Create Doc object\n",
    "\n",
    "## Explicity way\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "doc = Doc(Vocab(), words=[\"Hi\", \"there\"])\n",
    "# NOTES:\n",
    "#  - Vocab object: Storage container that provides vocabulary data, such as lexical types\n",
    "#      (adjective, verb, noun, and so on).\n",
    "#  - words argument: list of tokens to add to the Doc object being created.\n",
    "\n",
    "print(\"Explicit way: \", doc)\n",
    "\n",
    "## By default, SpaCy model return Doc object as we input the text into the nlp model\n",
    "doc = nlp(\"Hi there\")\n",
    "print(\"Return from Spacy NLP Model: \", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bbd3359-bc42-4606-a1fb-f10d2861bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. I\n",
      "1. want\n",
      "2. a\n",
      "3. green\n",
      "4. apple\n"
     ]
    }
   ],
   "source": [
    "# Iterating over token\n",
    "\n",
    "doc = nlp(\"I want a green apple\")\n",
    "for token in doc:\n",
    "    print(f\"{token.i}. {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09f049f3-c23e-431f-9cf2-7eb8ca458d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The children of apple: [a, green]\n",
      "The children of apple: [a, green]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">want</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">green</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a1507cb0c3b4ebe9cd0eae0fc0c9c31-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get children\n",
    "# NOTE: By default, SpaCY uses dependency grammar parsing. The structure of Head-Child in token: Head in the right side, Child in left side.\n",
    "# - Token.lefts ==> Return generator\n",
    "# - Token.children ==> Return generator\n",
    "\n",
    "doc = nlp(\"I want a green apple\")\n",
    "print(f\"The children of {doc[4]}: {list(doc[4].lefts)}\")\n",
    "print(f\"The children of {doc[4]}: {list(doc[4].children)}\")\n",
    "\n",
    "\n",
    "from spacy import displacy\n",
    "# Style as dependency\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "660f5399-2a05-40c0-a60f-a6ec8619f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A severe storm hit the beach.\n",
      "2 It started to rain.\n",
      "Type element Doc.sents:  <class 'spacy.tokens.span.Span'>\n",
      "The second sentence begins with a pronoun.\n",
      "Number of sentences end with a verb:  1\n"
     ]
    }
   ],
   "source": [
    "# Get sentence object\n",
    "# Doc.sents ==> Return generator, each element in span object\n",
    "\n",
    "doc = nlp(\"A severe storm hit the beach. It started to rain.\")\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    print(idx + 1, s.text)\n",
    "\n",
    "print(\"Type element Doc.sents: \", type(s))\n",
    "\n",
    "# Example: check whether the first word in the second sentence of the text being processed is a pronoun\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    if s[0].pos_ == 'PRON' and idx == 1:\n",
    "        print(\"The second sentence begins with a pronoun.\")\n",
    "\n",
    "# Example: find out how many sentences in the text end with a verb\n",
    "count = 0\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    if s[len(s) - 2].pos_ == 'VERB':\n",
    "        count += 1\n",
    "print(\"Number of sentences end with a verb: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ed8dbd-0fa3-43a0-b359-5d93a75bc17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk noun: \n",
      "The quick brown fox\n",
      "the lazy dog\n",
      "\n",
      "Manual way: \n",
      "The quick brown fox\n",
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# A noun (or phrase noun) that consists of a noun and its immediate dependents, such as adjectives, determiners, or other words\n",
    "#  that modify the noun.\n",
    "\n",
    "# Using Doc.noun_chunks ==> return Span object.\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumped over the lazy dog.\")\n",
    "\n",
    "print(\"Chunk noun: \")\n",
    "for s in doc.noun_chunks:\n",
    "    print(s.text)\n",
    "\n",
    "# Manual code for noun_chunks\n",
    "store = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        chunk = \"\"\n",
    "        for w in token.children:\n",
    "            if w.pos_ == \"DET\" or w.pos_ == \"ADJ\":\n",
    "                chunk = chunk + w.text + \" \"\n",
    "        chunk = chunk + token.text\n",
    "        store.append(chunk)\n",
    "\n",
    "print(\"\\nManual way: \")\n",
    "for s in store:\n",
    "    print(s)\n",
    "# NOTE: \n",
    "# - noun chunks can also include some other parts of speech, say, adverbs.\n",
    "# - the words used to modify noun (determiners and adjectives) are always the leftward syntactic children of the noun.\n",
    "#    So basically, we do not have to add \"if w.pos_ == \"DET\" or w.pos_ == \"ADJ\" part.\n",
    "# - the manual code returns string, but doc.noun_chunks returns Span object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd54dd8b-1d6c-486a-8d11-da33c37ef8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox', 'the lazy dog']\n"
     ]
    }
   ],
   "source": [
    "# Modification manual code that follows the previous notes.\n",
    "store = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        chunk = \"\"\n",
    "        for w in token.lefts:\n",
    "            chunk = chunk + w.text + \" \"\n",
    "\n",
    "        chunk = chunk + token.text\n",
    "        store.append(chunk)\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5c7234f-a3a3-41dc-8b69-cf6914bec13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Span object: a slice from a Doc object.\n",
    "#  Some example Span object: (1) slice doc: Doc[start:end], (2) element Doc.sents, (3) element Doc.noun_chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9072b-133e-4ac9-ba7e-158eafa79f1a",
   "metadata": {},
   "source": [
    "### Pipeline Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d573287c-cb0e-4fb2-be4b-91401d447362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'custom_lemma_setter', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Check what pipeline components are available\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9a04283-b107-40a5-9e68-23cb7943cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Text POS Dependency (disable)\n",
      "I PRON \n",
      "want VERB \n",
      "a DET \n",
      "green ADJ \n",
      "apple NOUN \n",
      ". PUNCT \n"
     ]
    }
   ],
   "source": [
    "# Disabling pipeline components (from model that already contain specific component)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser']) # Disable dependency parser\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Try to get dependency label of each token (it won't appears since it is disable)\n",
    "doc = nlp(\"I want a green apple.\")\n",
    "\n",
    "print(\"Text\", \"POS\", \"Dependency (disable)\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276f2f7-99ec-4d94-949a-af784a708245",
   "metadata": {},
   "source": [
    "# 4 - Extracting and Using Linguistic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a553051-6dfc-4393-aa0e-7b2e029b2020",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extracting and Generating Text with Part-of-Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "872a5feb-7f14-45c4-9cc6-4c8f659e7323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Pos Explain\n",
      "The DET determiner\n",
      "firm NOUN noun\n",
      "earned VERB verb\n",
      "$ SYM symbol\n",
      "1.5 NUM numeral\n",
      "milion NOUN noun\n",
      "in ADP adposition\n",
      "2017 NUM numeral\n",
      ". PUNCT punctuation\n",
      "\n",
      "Text Pos Tag Explain Tag\n",
      "The DET DT determiner\n",
      "firm NOUN NN noun, singular or mass\n",
      "earned VERB VBD verb, past tense\n",
      "$ SYM $ symbol, currency\n",
      "1.5 NUM CD cardinal number\n",
      "milion NOUN NN noun, singular or mass\n",
      "in ADP IN conjunction, subordinating or preposition\n",
      "2017 NUM CD cardinal number\n",
      ". PUNCT . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# Numeric, Symbolic, and Punctuation Tags\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The firm earned $1.5 milion in 2017.\")\n",
    "## Intro: Extract coarse-grained part-of-speech features\n",
    "print(\"Text\", \"Pos\", \"Explain\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))\n",
    "\n",
    "# NOTE:\n",
    "# - spacy.explain method returns a description for a given linguistic feature.\n",
    "# - Coarse-grained tagger distinguishes numerals, symbol and punctuation marks as\n",
    "#     individual categories. (It even recognizes million spelled out).\n",
    "\n",
    "print()\n",
    "## Intro: Extract Fine-grained part-of-speech features\n",
    "print(\"Text\", \"Pos\", \"Tag\", \"Explain Tag\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))\n",
    "\n",
    "# NOTE:\n",
    "# - The fine-grained tagging divides each category into subcategories.\n",
    "# - The coarse-grained category \"SYM\" has 3 subcategories: (1) $ for currency,\n",
    "#     (2) # for the number sign, and (3) SYM for all the other symbols (such as +, \n",
    "#     -, x, =,).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6e778-cc13-4641-a2be-3f47fcd2bb94",
   "metadata": {},
   "source": [
    "**Study Case: Extracting Descriptions of Money**\n",
    "\n",
    "Suppose we are interested in phrases that refer to an amount of money and start with a currency symbol. For example our script should pick out the phrase \"$1.5 milion\" from the sentence, not \"1.5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ace71067-6d1e-4b83-8455-93c1a136e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n"
     ]
    }
   ],
   "source": [
    "# Open file\n",
    "with open(\"data_spacy/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ae37a9c-8c91-4904-9d5d-2a00d45f50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1.5 million', '$1.2 million']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The firm earned $1.5 million in 2017\"\n",
    "input_text = \"The firm earned $1.5 million in 2017, in comparison with $1.2 million in 2016.\"\n",
    "\n",
    "doc = nlp(input_text)\n",
    "storage = []\n",
    "for token in doc:\n",
    "    phrase = \"\"\n",
    "    if token.tag_ == '$':\n",
    "        phrase = token.text\n",
    "        i = token.i + 1\n",
    "        while doc[i].tag_ == 'CD':\n",
    "            phrase += doc[i].text + \" \"\n",
    "            i += 1\n",
    "    phrase = phrase.rstrip()\n",
    "    if len(phrase) > 0:\n",
    "        storage.append(phrase)\n",
    "\n",
    "print(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252c5d7-0b12-4071-8259-0de7c137a00c",
   "metadata": {},
   "source": [
    "**Study Case: Turning Statements into Questions**\n",
    "\n",
    "Suppose your NLP application must be able to generate a question from a submitted statement. \n",
    "\n",
    "In this case the input:\n",
    "> I can promise it is worth your time.\n",
    "\n",
    "The output:\n",
    "> Can you really promise it is worth my time?\n",
    "\n",
    "The algorithm:\n",
    "1. Change the order of words in the original sentence from “subject + modal auxiliary verb + infinitive verb” to “modal auxiliary verb + subject + infinitive verb.”\n",
    "2. Replace the personal pronoun “I” (the sentence’s subject) with “you.”\n",
    "3. Replace the possessive pronoun “your” with “my.”\n",
    "4. Place the adverbial modifier “really” before the verb “promise” to emphasize the latter.\n",
    "5. Replace the punctuation mark “.” with “?” at the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f2cd72f-c40c-43b7-9be6-19eed26be418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can</td>\n",
       "      <td>AUX</td>\n",
       "      <td>MD</td>\n",
       "      <td>verb, modal auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>promise</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>verb, base form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worth</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>your</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>pronoun, possessive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>time</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text    pos   tag                                            explain\n",
       "0        I   PRON   PRP                                  pronoun, personal\n",
       "1      can    AUX    MD                              verb, modal auxiliary\n",
       "2  promise   VERB    VB                                    verb, base form\n",
       "3       it   PRON   PRP                                  pronoun, personal\n",
       "4       is    AUX   VBZ                  verb, 3rd person singular present\n",
       "5    worth    ADJ    JJ  adjective (English), other noun-modifier (Chin...\n",
       "6     your   PRON  PRP$                                pronoun, possessive\n",
       "7     time   NOUN    NN                             noun, singular or mass\n",
       "8        .  PUNCT     .                  punctuation mark, sentence closer"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze\n",
    "doc = nlp(\"I can promise it is worth your time.\")\n",
    "data = {'text': [],\n",
    "        'pos': [],\n",
    "        'tag': [],\n",
    "        'explain': []}\n",
    "\n",
    "for token in doc:\n",
    "    data['text'].append(token.text)\n",
    "    data['pos'].append(token.pos_)\n",
    "    data['tag'].append(token.tag_)\n",
    "    data['explain'].append(spacy.explain(token.tag_))\n",
    "\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f90b2d-4795-4d4a-aaf1-a43e7389e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you really promise it is worth my time?\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I can promise it is worth your time.\")\n",
    "sent = ''\n",
    "\n",
    "# Inversion Auxiliary Verb with Personal Pronoun\n",
    "for i, token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':\n",
    "        sent = doc[i+1].text.capitalize() + \" \" + doc[i].text + \" \" + doc[i+2:].text\n",
    "        break\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Replace Personal Pronouns: I into you\n",
    "for i, token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP' and token.text == 'I':\n",
    "        sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
    "        break\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Replace Possessive Pronouns: your into my\n",
    "for i, token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP$' and token.text == 'your':\n",
    "        sent = doc[:i].text + ' my ' + doc[i+1:].text\n",
    "        break\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Adding \"really\" in the middle\n",
    "for i, token in enumerate(doc):\n",
    "    if token.tag_ == \"VB\":\n",
    "        sent = doc[:i].text + ' really ' + doc[i:].text\n",
    "        break\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Adding \"?\" at the end\n",
    "sent = doc[:len(doc)-1].text + '?'\n",
    "\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c3c17-c08d-40c0-ae12-26abf7c68e4a",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "This script is a good start, but it won’t work with every submitted statement. For example, the statement might contain a personal pronoun other than “I,” but our script doesn’t explicitly check for that. Also, some sentences don’t contain auxiliary verbs, like the sentence “I love eating ice cream.” In those cases, we’d have to use the word “do” to form the question instead of a word like “can” or “should,” like this: “Do you really love eating ice cream?” But if the sentence contains the verb “to be,” as in the sentence “I am sleepy,” we’d have to move that verb to the front, like this: “Are you sleepy?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5911808-7f23-4323-84db-00e804846e75",
   "metadata": {},
   "source": [
    "### Using Syntactic Dependency Labels in Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a25353a7-10e4-4cc7-b2ea-8a03e9069e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dependency</th>\n",
       "      <th>explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>nominal subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>direct object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>nominal subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>know</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>me</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>direct object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text    pos  tag dependency          explain\n",
       "0     I   PRON  PRP      nsubj  nominal subject\n",
       "1  know   VERB  VBP       ROOT             root\n",
       "2   you   PRON  PRP       dobj    direct object\n",
       "3     .  PUNCT    .      punct      punctuation\n",
       "4   You   PRON  PRP      nsubj  nominal subject\n",
       "5  know   VERB  VBP       ROOT             root\n",
       "6    me   PRON  PRP       dobj    direct object\n",
       "7     .  PUNCT    .      punct      punctuation"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example explain something more deeply by looking its dependency\n",
    "doc = nlp(\"I know you. You know me.\")\n",
    "\n",
    "data = {\n",
    "    'text': [],\n",
    "    'pos': [],\n",
    "    'tag': [],\n",
    "    'dependency': [],\n",
    "    'explain': [],\n",
    "}\n",
    "for token in doc:\n",
    "    data['text'].append(token.text)\n",
    "    data['pos'].append(token.pos_)\n",
    "    data['tag'].append(token.tag_)\n",
    "    data['dependency'].append(token.dep_)\n",
    "    data['explain'].append(spacy.explain(token.dep_))\n",
    "\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad9cb6-7414-41b5-8ebb-3bc7961554b9",
   "metadata": {},
   "source": [
    "NOTE: In some case, we need to see the dependency of each token to get token that we really needed. For example in this case, if we want to extract \"you\" that represent as direct object we by looking only its pos tag is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b4e33-0f30-4575-8908-5865c88e587c",
   "metadata": {},
   "source": [
    "**Study Case: Deciding What Question a Chatbot Should Ask**\n",
    "\n",
    "Suppose that we want to create chatbot that asks a yes/no question. The flow of program is shown as below:\n",
    "\n",
    "![images](data_spacy/images/chatbot-green-apple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92c079e4-9156-4e69-8498-af117ceae5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dependency</th>\n",
       "      <th>explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>nominal subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>want</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>green</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>adjectival modifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>direct object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text   pos  tag dependency              explain\n",
       "0      I  PRON  PRP      nsubj      nominal subject\n",
       "1   want  VERB  VBP       ROOT                 root\n",
       "2      a   DET   DT        det           determiner\n",
       "3  green   ADJ   JJ       amod  adjectival modifier\n",
       "4  apple  NOUN   NN       dobj        direct object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze\n",
    "doc = nlp(\"I want a green apple\")\n",
    "\n",
    "data = {\n",
    "    'text': [],\n",
    "    'pos': [],\n",
    "    'tag': [],\n",
    "    'dependency': [],\n",
    "    'explain': [],\n",
    "}\n",
    "for token in doc:\n",
    "    data['text'].append(token.text)\n",
    "    data['pos'].append(token.pos_)\n",
    "    data['tag'].append(token.tag_)\n",
    "    data['dependency'].append(token.dep_)\n",
    "    data['explain'].append(spacy.explain(token.dep_))\n",
    "\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf1de240-c0be-4882-8d28-fea0e8900775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find chunk\n",
    "def find_chunk(doc):\n",
    "    chunk = ''\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.dep_ == 'dobj':\n",
    "            shift = len([w for w in token.children])\n",
    "            # Slicing from leftmost child into current token.\n",
    "            chunk = doc[i-shift:i+1]\n",
    "            break\n",
    "    return chunk\n",
    "\n",
    "def determine_question_type(chunk):\n",
    "    \"\"\"\n",
    "    Determine if the question yes/no or information type.\n",
    "\n",
    "    If the chunk contains adjective modifier \"amod\", then the\n",
    "      question is yes/no type. Otherwise information type.\n",
    "    \"\"\"\n",
    "    question_type = 'yesno'\n",
    "    for token in chunk:\n",
    "        if token.dep_ == 'amod':\n",
    "            question_type = 'info'\n",
    "    return question_type\n",
    "\n",
    "def generate_question(doc, question_type):\n",
    "    sent = ''\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
    "            sent = 'do ' + doc[i].text\n",
    "            sent = sent + ' ' + doc[i+1:].text\n",
    "            break\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.tag_ == 'PRP' and token.text == 'I':\n",
    "            sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
    "            break\n",
    "    doc = nlp(sent)\n",
    "    \n",
    "    if question_type == 'info':\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.dep_ == 'dobj':\n",
    "                sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
    "                break\n",
    "    if question_type == 'yesno':\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.dep_ == 'dobj':\n",
    "                sent = doc[:i-1].text + ' a red ' + doc[i:].text\n",
    "                break\n",
    "\n",
    "    doc = nlp(sent)\n",
    "    sent = doc[0].text.capitalize() + ' ' + doc[1:len(doc)].text + '?'\n",
    "\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86dffd88-ded3-4652-8157-6433ce4bca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I want a green apple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do you want a green one?\n"
     ]
    }
   ],
   "source": [
    "text_input = input()\n",
    "\n",
    "doc = nlp(text_input)\n",
    "chunk = find_chunk(doc)\n",
    "if str(chunk) == '':\n",
    "    print('The sentence does not contain a direct object.')\n",
    "else:\n",
    "    question_type = determine_question_type(chunk)\n",
    "    question = generate_question(doc, question_type)\n",
    "    print(question)\n",
    "\n",
    "# Try input:\n",
    "# I want a green apple => Why do you want a green one?\n",
    "# I want an apple => Do you want a red apple?\n",
    "# I want... => The sentence does not contain a direct object.\n",
    "# empty string => You did not submit a sentence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f7fed-0614-4573-80a7-f671c8472030",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5 - Working with Word Vectors\n",
    "\n",
    "- Word vector space can be imagine as a cloud in which the vectors of words with similar meanings are located nearby.\n",
    "- A word vector space uses the distance between vectors to quantify and categorize semantic similarities.\n",
    "\n",
    "  NOTE, recall that main key factors that influence position of a word vector are:\n",
    "  \n",
    "        - Co-occurence in similar contexts during training model.\n",
    "        - Frequency of co-occurence during training model.\n",
    "\n",
    "- To compare a single token with an entire sentence, spaCy averages the sentence’s word vectors to generate an entirely new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8843ba30-1190-451d-bb26-14f14d9e0ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want a green apple <-> I want a yellow manggo 0.9698261546195504\n",
      "green apple <-> yellow manggo 0.6900869011878967\n",
      "green <-> yellow 0.7846790552139282\n"
     ]
    }
   ],
   "source": [
    "# Using Doc.similarity method (It can be used in Span and Token too)\n",
    "#  similarity(object) ==> return float; object can be Doc, Span, and Token.\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# NOTE: since en_core_web_sm do not contain similarity method (because it's small version)\n",
    "#        we use its medium version.\n",
    "\n",
    "# Example Doc\n",
    "doc = nlp('I want a green apple')\n",
    "doc1 = nlp(\"I want a yellow manggo\")\n",
    "print(doc, \"<->\", doc1, doc.similarity(doc1))\n",
    "\n",
    "# Example Span\n",
    "span = doc[3:]\n",
    "span1 = doc1[3:]\n",
    "print(span, \"<->\", span1, span.similarity(span1))\n",
    "\n",
    "# Example Token\n",
    "token = doc[3]\n",
    "token1 = doc1[3]\n",
    "print(token, \"<->\", token1, token.similarity(token1))\n",
    "\n",
    "# NOTE: \n",
    "#   - The similarity method will calculate semantic similarity for you, \n",
    "#       but for the results of that calculation to be useful, \n",
    "#       you need to choose the right keywords to compare.\n",
    "#   - We can extract the keyword and then checking the similarity and the search phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6edbf9-160a-4f81-8b1e-990a892e461e",
   "metadata": {},
   "source": [
    "**Study Case: Semantic Similarity for Categorization Tasks**\n",
    "\n",
    "\n",
    "Suppose that we want to know that if a sentence is related with \"fruits\". The input is:\n",
    "\n",
    "\n",
    "\"I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48636c43-5883-4678-9421-5c30d86f5f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to buy this beautiful book at the end of the week.\n",
      "similarity to fruit is 0.26244229078292847 \n",
      "\n",
      "Sales of citrus have increased over the last year.\n",
      "similarity to fruit is 0.2754635810852051 \n",
      "\n",
      "How much do you know about this type of tree?\n",
      "similarity to fruit is 0.3160061538219452 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I want to buy this beautiful book at the end of the week.\" \n",
    "          \"Sales of citrus have increased over the last year.\" \n",
    "          \"How much do you know about this type of tree?\")\n",
    "token = nlp('fruit')[0]\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('similarity to', token.text, 'is', token.similarity(sent), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e8f8a-c007-40dc-8fac-504d9db40ed4",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- Remember that spaCy will average the sentence vector based on each words.\n",
    "    This can lead a problem. If the text that we are averaging is very large,\n",
    "    the most important words might have little to no effect on the syntactic\n",
    "    similarity value.\n",
    "- To get more accurate results, we could extract the important parts of sentence then see its similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88204149-bffc-429d-9ec4-a6b24f8e3974",
   "metadata": {},
   "source": [
    "**Study Case: Extracting Nouns as a Preprocessing Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deb6183a-c905-4c1c-aa23-f6accb848d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.15416097214471128, 1: 0.32236731321399387, 2: 0.49644641541472323}\n"
     ]
    }
   ],
   "source": [
    "token = nlp('fruit')[0]\n",
    "doc = nlp(\"I want to buy this beautiful book at the end of the week.\" \n",
    "          \"Sales of citrus have increased over the last year.\" \n",
    "          \"How much do you know about this type of tree?\")\n",
    "\n",
    "similarity = {}\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    noun_span_list = [sent[j].text for j in range(len(sent)) if sent[j].pos_ == 'NOUN']\n",
    "    noun_span_str = ' '.join(noun_span_list)\n",
    "    noun_span_doc = nlp(noun_span_str)\n",
    "    similarity.update({i: token.similarity(noun_span_doc)})\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42c4f0-122e-4cdb-99c3-4a813badeda9",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- this time the level of the similarity with the word “fruits” is higher for each sentence. But the overall results look similar: the similarity of the first sentence is the lowest, whereas the similarity of the other two are much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ebf98fa-1ae5-4c97-be77-879af1b42347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book <-> fruit 0.10111276732078336\n",
      "end <-> fruit 0.12615228452479088\n",
      "week <-> fruit 0.07047647225810715\n",
      "Sales <-> fruit 0.041778367824821624\n",
      "citrus <-> fruit 0.7326242077139979\n",
      "year <-> fruit 0.07115306346492278\n",
      "type <-> fruit 0.2448628913815218\n",
      "tree <-> fruit 0.5153199440186008\n",
      "{0: 0.12615228452479088, 1: 0.7326242077139979, 2: 0.5153199440186008}\n",
      "[(0, ['book', 'end', 'week']), (1, ['Sales', 'citrus', 'year']), (2, ['type', 'tree'])]\n"
     ]
    }
   ],
   "source": [
    "# Try using the highest level of similarity of each extracted noun.\n",
    "\n",
    "## Extract noun\n",
    "words = []\n",
    "similarity = {}\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    noun_span_list = [sent[j]. text for j in range(len(sent)) if sent[j].pos_ == 'NOUN']\n",
    "    words.append((i, noun_span_list))\n",
    "\n",
    "for i, ws in words:\n",
    "    max_value = 0\n",
    "    current = 0\n",
    "\n",
    "    for w in ws:\n",
    "        w_token = nlp(w)\n",
    "        current = token.similarity(w_token)\n",
    "        print(w_token, \"<->\", token, current)\n",
    "        if current > max_value:\n",
    "            max_value = current\n",
    "\n",
    "    similarity.update({i: max_value})\n",
    "\n",
    "print(similarity)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd828952-9606-4506-a5cf-07849d40cf7c",
   "metadata": {},
   "source": [
    "**Study Case: Extracting and Comparing Named Entities**\n",
    "\n",
    "The idea is extracting only the words marked as named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0d5eecb-56e6-4488-9de1-c507a118babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'Search', 'Google', 'each', 'day']\n",
      "['Microsoft', 'Windows', 'Microsoft']\n",
      "['Titicaca', 'Andes']\n",
      "doc1 is similar to doc2: 0.5329812636743593\n",
      "doc1 is similar to doc3: 0.12925641941282406\n",
      "doc2 is similar to doc3: 0.001502677465755575\n"
     ]
    }
   ],
   "source": [
    "# Get sample data 1\n",
    "text1 = \"Google Search, often referred to as simply Google, is the most\" +\\\n",
    "        \" used search engine nowadays. It handles a huge number of searches each day.\"\n",
    "doc1 = nlp(text1)\n",
    "\n",
    "# Get sample data 2\n",
    "text2 = \"Microsoft Windows is a family of proprietary operating systems\" +\\\n",
    "        \" developed and sold by Microsoft. The company also produces a wide range of\" +\\\n",
    "        \" other software for desktops and servers.\"\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "# Get sample data 3\n",
    "text3 = \"Titicaca is a large, deep, mountain lake in the Andes.\" +\\\n",
    "        \" It is known as the highest navigable lake in the world.\"\n",
    "doc3 = nlp(text3)\n",
    "\n",
    "docs = [doc1, doc2, doc3]\n",
    "spans = {}\n",
    "\n",
    "# Extracting keywords each document\n",
    "for j, doc in enumerate(docs):\n",
    "    named_entity_span = [doc[i].text for i in range(len(doc)) if doc[i].ent_type != 0]\n",
    "    print(named_entity_span)\n",
    "    named_entity_span = \" \".join(named_entity_span)\n",
    "    named_entity_span = nlp(named_entity_span)\n",
    "    spans.update({j: named_entity_span})\n",
    "\n",
    "# Similarity calculation\n",
    "print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))\n",
    "print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))\n",
    "print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143d9d7-34d4-469c-ab44-9667fd7e873f",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- It probably the words “Google” and “Microsoft” have been found more often in the same texts of the training text corpus rather than in the company of the words “Titicaca” and “Andes.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d1013-e8b3-4b80-9071-ed5fc7599610",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7 - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf745bc-50d7-4549-bb6d-ca90bd07fd7b",
   "metadata": {},
   "source": [
    "**Visualizing Dependency Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41a3d93c-4b13-4a9b-943f-4221cab8c187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"be3f728e42ad4f34bb0b317895827dd5-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">want</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Greek</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">pizza.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-be3f728e42ad4f34bb0b317895827dd5-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-be3f728e42ad4f34bb0b317895827dd5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-be3f728e42ad4f34bb0b317895827dd5-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-be3f728e42ad4f34bb0b317895827dd5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-be3f728e42ad4f34bb0b317895827dd5-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-be3f728e42ad4f34bb0b317895827dd5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-be3f728e42ad4f34bb0b317895827dd5-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-be3f728e42ad4f34bb0b317895827dd5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u\"I want a Greek pizza.\")\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe71d78-c7c5-43f2-b39b-d8a7595744f3",
   "metadata": {},
   "source": [
    "**Visualizing Named Entity Recognizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6346fa08-b438-420d-914c-e88e3b0b24d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I want a \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Greek\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " pizza.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u\"I want a Greek pizza.\")\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef698f-3d78-4ebe-818e-e66439546a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
