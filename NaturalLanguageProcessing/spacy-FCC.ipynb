{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef06b42-16d0-48fe-8680-b554d2eacf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed7faa5-fa4c-4ed1-96d0-68308b37ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NLP object (load the model that installed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cdb3ee-98d3-4105-b004-3e85851155f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72e4e7-c459-4b3d-b1aa-cc7cd4019e3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### What is Natural Language Processing\n",
    "- Natural Language Processing is a subfield of artificial intelligence that tries to **process and analyze natural language data.**\n",
    "  \n",
    "NOTE: Natural language is a language that developed naturally through use.\n",
    "\n",
    "The idea: Since we already know semantic and grammar rules of human language, then we can build applications that can progammatically understand utterances in that language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd11414-e853-4b18-9d08-c30e02b25b87",
   "metadata": {},
   "source": [
    "### How can Computers Understand Language\n",
    "- Since computer (or machine) only understand number, we need to convert language words into numbers. This process called **Word Embedding**.\n",
    "- Word Embedding concept: **Mapping the words to vectors of real numbers that distribute the meaning of each word** between the coordinates of the corresponding word vector. NOTE:\n",
    "    - Words that similar (in machine perspectives) if their word vector are nearby.\n",
    "    - Two words are distributed nearby in the vector space based on **the contextual similarity** of their usage in a large corpus of text.\n",
    "      \n",
    "      NOTE: Key factors that influence are (1) Co-occurence in Similar Contexts and (2) Frequency of Co-occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8d1fb-9c1b-411e-87a2-348059c288bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Getting Started with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace71067-6d1e-4b83-8455-93c1a136e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n"
     ]
    }
   ],
   "source": [
    "# Open file\n",
    "with open(\"data_spacy/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8843ba30-1190-451d-bb26-14f14d9e0ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create doc object\n",
    "doc = nlp(text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529c52bd-7c5c-45e7-acb3-2f043e57dfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
       "\n",
       "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
       "\n",
       "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
       "\n",
       "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
       "\n",
       "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd92353-de05-40df-a1d8-c2ae63681efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length:  3525\n",
      "Doc object length:  652\n"
     ]
    }
   ],
   "source": [
    "# Compare length text and Doc object\n",
    "print(\"Text length: \", len(text))\n",
    "print(\"Doc object length: \", len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9c2c99-da92-4465-a19b-7bed1c2c4b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text element: \n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "U\n",
      "n\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      "\n",
      "Doc object element: \n",
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(\n",
      "U.S.A.\n",
      "or\n",
      "USA\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Compare element of text and Doc object\n",
    "print(f\"Text element: \")\n",
    "for token in text[0:10]:\n",
    "    print(token)\n",
    "\n",
    "print(\"\\nDoc object element: \")\n",
    "for token in doc[0:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0c216b-91a3-468c-8651-74d365ec4cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split\n",
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(U.S.A.\n",
      "or\n",
      "USA),\n",
      "commonly\n",
      "known\n",
      "\n",
      "Tokenization rules:\n",
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(\n",
      "U.S.A.\n",
      "or\n",
      "USA\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Tokenization based on rules spacy vs string split\n",
    "print(\"Text split\")\n",
    "for token in text.split()[:10]:\n",
    "    print(token)\n",
    "\n",
    "print(\"\\nTokenization rules:\")\n",
    "for token in doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5926fef5-fc8e-44c4-8c9d-fa740806858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n",
      "2. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j]\n",
      "3. At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d]\n",
      "4. The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22]\n",
      "5. With a population of more than 331 million people, it is the third most populous country in the world.\n",
      "6. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "\n",
      "7. Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century.\n",
      "8. The United States emerged from the thirteen British colonies established along the East Coast.\n",
      "9. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence.\n",
      "10. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent.\n",
      "\n",
      "Span object:  <class 'spacy.tokens.span.Span'>\n",
      "Token object:  <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Try to get sentence-based tokenization Doc object\n",
    "# Note: using \"sents\" attribute. The Doc.sents return generator.\n",
    "#        Each element of generator is Span object.\n",
    "#        The Span object contains Token objects\n",
    "\n",
    "for idx, sent in enumerate(list(doc.sents)[:10]):\n",
    "    print(f\"{idx + 1}. {sent}\")\n",
    "\n",
    "print()\n",
    "print(\"Span object: \", type(sent))\n",
    "print(\"Token object: \", type(sent[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3080f-cb2a-47ad-864a-11a27976464d",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "- Doc object contains individual token (based on tokenization rules), but the text input contains individual character.\n",
    "- By default, Doc object will word-based tokenize the input.\n",
    "- Doc, Span, or Token object have their own meta-data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1045a-b5ad-4a0e-8bb5-6d7cd547a3c1",
   "metadata": {},
   "source": [
    "## Extract meta-data from Token Object\n",
    "\n",
    "In this example we use Token object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb50a7c-16bc-4722-b697-4286aa1209f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main sentence:\n",
      " The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Tokens object properties\n",
    "\n",
    "sentence1 = list(doc.sents)[0]\n",
    "print(\"Main sentence:\\n\", sentence1.text)\n",
    "print(type(sentence1))\n",
    "\n",
    "token1 = sentence1[12]\n",
    "print(type(token1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51956576-8487-4b1d-9daa-b734b866ff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'known'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get text (string format type)\n",
    "#  Use \"text\" properties.\n",
    "\n",
    "token1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d8f3bf-2acb-4b88-a5fa-6c9181be465e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "States"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get which word (Token object) it is governed by.\n",
    "#  Return Token object\n",
    "token1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7838e41c-bf7d-4419-8862-6d2d44181c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commonly"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the leftmost token of this token's syntactic descendants.\n",
    "#  Return Token object\n",
    "token1.left_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ab5609-4d5d-44ca-af09-74a9f79456c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "America"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the rightmost token of this token's syntactic descendants.\n",
    "#  Return Token object\n",
    "token1.right_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "357da34d-a02e-4f25-b520-a699280115b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "GPE\n"
     ]
    }
   ],
   "source": [
    "# Entity Type\n",
    "print(sentence1[2].ent_type) # Return integer that corresponds to an entity type.\n",
    "print(sentence1[2].ent_type_) # Return string name entity type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88932f9-3597-4341-82ce-19baa837f6b9",
   "metadata": {},
   "source": [
    "Some explanations:\n",
    "- PERSON: People, Including Fictional.\n",
    "- NORP: Nationalities or religious or political groups. \n",
    "- FAC: Buildings, airports, highways, bridges, etc.\n",
    "- ORG: Companies, agencies, institutions, etc.\n",
    "- GPE: Countries, cities, states.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW: Named documents made into laws.\n",
    "- LANGUAGE: Any named language.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- TIME: Times smaller than a day.\n",
    "- PERCENT: Percentage, including ”%“.\n",
    "- MONEY: Monetary values, including unit.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- ORDINAL: “first”, “second”, etc.\n",
    "- CARDINAL: Numerals that do not fall under another type.\n",
    "der another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d058e3-300d-4b40-bd29-eff377a7438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known O\n",
      "known 2\n",
      "States I\n",
      "States 1\n"
     ]
    }
   ],
   "source": [
    "# IOB Entity Method --> IOB code of named entity tag.\n",
    "#   “B” means the token begins an entity, \n",
    "#   “I” means it is inside an entity, \n",
    "#   “O” means it is outside an entity, \n",
    "#   and \"\" means no entity tag is set.\n",
    "\n",
    "print(token1, token1.ent_iob_) # Return string name entity type.\n",
    "print(token1, token1.ent_iob) # Return integer that corresponds to an entity type.\n",
    "print(sentence1[2], sentence1[2].ent_iob_)\n",
    "print(sentence1[2], sentence1[2].ent_iob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb2d136-292f-4d24-a0d8-4b7327acc307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know\n"
     ]
    }
   ],
   "source": [
    "# Lemma --> Get base form of token, with no inflectional suffixes.\n",
    "print(token1.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32490a14-43e0-4182-a48a-03d65e8a6a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known\n",
      "Aspect=Perf|Tense=Past|VerbForm=Part\n"
     ]
    }
   ],
   "source": [
    "# Morph Analysis\n",
    "#  Return MorphAnalysis object.\n",
    "\n",
    "print(token1)\n",
    "print(token1.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13a97c-8073-4ba1-a2eb-f6be9b3c160a",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- Aspect refers to how an action, event, or state, expressed by a verb.\n",
    "- Aspect=Perf ==> Perfective Aspect, indicates the action is completed.\n",
    "- Tense=Past ==> Past Tense\n",
    "- VerbForm=Part ==> Part stands for participle, participles are typically used in conjunction with auxiliary verbs to form different tenses or aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb5b901f-9566-4930-8611-a562506d6c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "# Coarse-grained part-of-speech from the Universal POS tag set\n",
    "print(token1.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eecdad0-506e-4c73-b387-5f455bb1427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acl\n"
     ]
    }
   ],
   "source": [
    "# Syntatic dependency relation\n",
    "print(token1.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "027a0740-0fd1-4fad-b3a1-8de0e9734361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "# Language of the parent document's vocabulary\n",
    "print(token1.lang_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9e28939-4925-41f1-8c3e-3dc47c050977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike enjoys playing football.\n"
     ]
    }
   ],
   "source": [
    "# Try another example\n",
    "text = \"Mike enjoys playing football.\"\n",
    "doc2 = nlp(text)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb310ba-1dd5-4ff7-b2e6-17de66056558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike PROPN nsubj\n",
      "enjoys VERB ROOT\n",
      "playing VERB xcomp\n",
      "football NOUN dobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4b166aa-64c8-448a-bb97-fbc3a2b88945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4975446ef70643baa1ea119a6f5ea570-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Mike</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enjoys</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">playing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">football.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4975446ef70643baa1ea119a6f5ea570-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4975446ef70643baa1ea119a6f5ea570-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4975446ef70643baa1ea119a6f5ea570-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4975446ef70643baa1ea119a6f5ea570-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4975446ef70643baa1ea119a6f5ea570-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4975446ef70643baa1ea119a6f5ea570-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize it\n",
    "from spacy import displacy\n",
    "# Style as dependency\n",
    "displacy.render(doc2, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cef62d23-9d2c-4c2d-8b36-9287feb7764f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mike\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " enjoys playing football.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Style based on entities\n",
    "displacy.render(doc2, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e22577b5-1cf5-4880-a3d8-9ddbaa46d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States of America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.A.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "), commonly known as \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ") or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", is a country primarily located in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". It consists of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    50\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " states, a federal district, \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    five\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " major unincorporated territories, \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    326\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " reservations, and some minor possessions.[j] At \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.8 million square miles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    9.8 million square kilometers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       "), it is the world's third- or \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    fourth\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       "-largest country by total area.[d] \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " shares significant land borders with \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Canada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " to the north and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mexico\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " to the south, as well as limited maritime borders with the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bahamas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cuba\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and Russia.[22] With a population of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than 331 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people, it is the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    third\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " most populous country in the world. The national capital is \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Washington\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    D.C.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and the most populous city is \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".<br><br>\n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paleo-Indians\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " migrated from \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Siberia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " to the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " mainland \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    at least 12,000 years ago\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " colonization began in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the 16th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " emerged from the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    thirteen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " colonies established along \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the East Coast\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Disputes over taxation and political representation with \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " led to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the American Revolutionary War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (1775â€“1783), which established independence. In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the late 18th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " began expanding across \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", gradually obtaining new territories, sometimes through war, frequently displacing \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Native Americans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", and admitting new states; by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1848\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " spanned the continent. Slavery was legal in the southern \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " until \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the second half of the 19th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " when \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the American Civil War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " led to its abolition. \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Spanishâ€“American War and World War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " I established the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " as a world power, a status confirmed by the outcome of \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World War II\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ".<br><br>During \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Cold War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " fought \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Korean War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Vietnam War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " but avoided direct military conflict with \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Soviet Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". The \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " superpowers competed in \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Space Race\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       ", culminating in the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1969\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " spaceflight that \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " landed humans on the Moon. \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Soviet Union's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " dissolution in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1991\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " ended \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Cold War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ", leaving \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " as the world's sole superpower.<br><br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a federal republic and a representative democracy with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " separate branches of government, including a bicameral legislature. It is a founding member of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United Nations\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Bank\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    International Monetary Fund\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Organization of American States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NATO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and other international organizations. It is a permanent member of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United Nations Security Council\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    centuries\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.<br><br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a highly developed country, accounts for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    approximately a quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of global GDP, and is the world's largest economy. By value, \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is the world's largest importer and the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       "-largest exporter of goods. Although its population is \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only 4.2%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " of the world's total, it holds \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    29.4%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " of the total wealth in the world, the largest share held by any country. Making up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than a third\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Entities doc model (from data imported)\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77f99e-54a4-449a-882e-e05e8b5afc54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Word Vectors and spaCy\n",
    "\n",
    "> Word vectors (or word embeddings) are numerical representations of words in multidimensional space through matrices.\n",
    "\n",
    "The word similarity:\n",
    "> The word similar means that the word that occurs frequently alongside of it. Sometimes it can be synonym or sometimes is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d584744-ef02-45b1-abd0-424d690fb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# Find location model on local:\n",
    "# nlp._path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2947d901-f734-4779-b818-c1af05525049",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_spacy/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d83c78d-a6f7-4838-adc4-21d7ad593833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac95ad85-cf9a-4437-9cd7-1cee9914a331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country—0,467', 'nationâ\\x80\\x99s', 'countries-', 'continente', 'Carnations', 'pastille', 'бесплатно', 'Argents', 'Tywysogion', 'Teeters']\n"
     ]
    }
   ],
   "source": [
    "# Example 1 (find the top n similar word from trained word in model)\n",
    "your_word = \"country\"\n",
    "\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]),\n",
    "    n=10\n",
    ")\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a227367-4676-45be-853c-fd26db6d7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.691649353055761\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d608f3d6-3756-4a1a-b0ae-4a2a65a74f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> The Empire State Building is in New York. 0.1766669125394067\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "doc3 = nlp(\"The Empire State Building is in New York.\")\n",
    "print(doc1, \"<->\", doc3, doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d980cc7-9237-419f-9431-62279e60bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy oranges. <-> I enjoy apples. 0.9775700747747101\n"
     ]
    }
   ],
   "source": [
    "# Example 4\n",
    "doc4 = nlp(\"I enjoy oranges.\")\n",
    "doc5 = nlp(\"I enjoy apples.\")\n",
    "print(doc4, \"<->\", doc5, doc4.similarity(doc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa206711-a1e4-4f04-bd0e-ac7e528823b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy oranges. <-> I enjoy burgers. 0.9628306076251026\n"
     ]
    }
   ],
   "source": [
    "# Example 6\n",
    "doc6 = nlp(\"I enjoy burgers.\")\n",
    "print(doc4, \"<->\", doc6, doc4.similarity(doc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edfc620a-efd6-458b-a6c3-656de7679b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salty fries <-> hamburgers 0.6938489079475403\n"
     ]
    }
   ],
   "source": [
    "# Example 7\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8d7d7-4955-4a32-a8ca-0a089aed526f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Standard Pipeline (Getting Started)\n",
    "\n",
    "There are two ways to adding custom features to Spacy language model:\n",
    "1. Rules-based approach\n",
    "2. Machine learning-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cf0e3-6262-4ccd-aaae-a0e10238acce",
   "metadata": {},
   "source": [
    "**Attribute Rulers:**\n",
    "- Dependency Parser\n",
    "- EntityLinker\n",
    "- EntityRecognizer\n",
    "- EntityRuler\n",
    "- Lemmatizer\n",
    "- Morpholog\n",
    "- SentenceRecognizer\n",
    "- Sentencizer\n",
    "> It is Sentence Tokenization. The sentence-level tokenization based on language rules.\n",
    "- SpanCategorizer\n",
    "- Tagger\n",
    "- TextCategorizer\n",
    "- Tok2Vec\n",
    "- Tokenizer\n",
    "- TrainablePipe\n",
    "- Transformer\n",
    "\n",
    "**Matcher:**\n",
    "- DependencyMatcher\n",
    "- Matcher\n",
    "- PhraseMatcher\n",
    "\n",
    "NOTE: It may new Attribute or Matcher is being added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ed211e6-3b94-41df-8a10-200f26c73c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n",
      "Blank Pipeline:\n",
      " {'summary': {}, 'problems': {}, 'attrs': {}}\n",
      "\n",
      "After Adding Pipeline: \n",
      " {'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'], 'requires': [], 'scores': ['sents_f', 'sents_p', 'sents_r'], 'retokenizes': False}}, 'problems': {'sentencizer': []}, 'attrs': {'doc.sents': {'assigns': ['sentencizer'], 'requires': []}, 'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []}}}\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate how to add pipes\n",
    "\n",
    "nlp = spacy.blank(\"en\") # Create nlp blank model \"english\" tokenizer\n",
    "print(type(nlp))\n",
    "print(\"Blank Pipeline:\\n\", nlp.analyze_pipes())\n",
    "\n",
    "# Add new Pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Add new Pipeline (Using class construction)\n",
    "# from spacy.pipeline import Sentencizer\n",
    "# sentencizer = Sentencizer()\n",
    "# nlp.add_pipe(sentencizer)\n",
    "\n",
    "\n",
    "print(\"\\nAfter Adding Pipeline: \\n\", nlp.analyze_pipes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed675716-2e12-4350-adcc-f0020d8dbf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust pipeline: \n",
      " {'summary': {'tok2vec': {'assigns': ['doc.tensor'], 'requires': [], 'scores': [], 'retokenizes': False}, 'tagger': {'assigns': ['token.tag'], 'requires': [], 'scores': ['tag_acc'], 'retokenizes': False}, 'parser': {'assigns': ['token.dep', 'token.head', 'token.is_sent_start', 'doc.sents'], 'requires': [], 'scores': ['dep_uas', 'dep_las', 'dep_las_per_type', 'sents_p', 'sents_r', 'sents_f'], 'retokenizes': False}, 'attribute_ruler': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': False}, 'lemmatizer': {'assigns': ['token.lemma'], 'requires': [], 'scores': ['lemma_acc'], 'retokenizes': False}, 'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'], 'requires': [], 'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'], 'retokenizes': False}}, 'problems': {'tok2vec': [], 'tagger': [], 'parser': [], 'attribute_ruler': [], 'lemmatizer': [], 'ner': []}, 'attrs': {'doc.sents': {'assigns': ['parser'], 'requires': []}, 'doc.ents': {'assigns': ['ner'], 'requires': []}, 'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}, 'token.is_sent_start': {'assigns': ['parser'], 'requires': []}, 'token.head': {'assigns': ['parser'], 'requires': []}, 'token.ent_iob': {'assigns': ['ner'], 'requires': []}, 'token.dep': {'assigns': ['parser'], 'requires': []}, 'token.tag': {'assigns': ['tagger'], 'requires': []}, 'token.ent_type': {'assigns': ['ner'], 'requires': []}, 'token.lemma': {'assigns': ['lemmatizer'], 'requires': []}}}\n"
     ]
    }
   ],
   "source": [
    "# Analyze robust pipeline\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Robust pipeline: \\n\", nlp2.analyze_pipes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "680a8d3f-9c74-4639-b009-26681d16f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1. This is a sentence.\n",
      "2. This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "# Try Sentencizer Pipeline\n",
    "# NOTE: This example must tokenize sentences into two sentence (token).\n",
    "\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "\n",
    "if len(list(doc.sents)) == 2:\n",
    "    print(True)\n",
    "\n",
    "for i, token in enumerate(list(doc.sents)):\n",
    "    print(f\"{i + 1}. {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcfa5d-f060-41c7-8124-740a67cfbd14",
   "metadata": {},
   "source": [
    "# Using SpaCy EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a7f20f5-9a0a-46a1-bb92-bda55a6e9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9965c845-e1ad-47e5-98c2-9309db4cd66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bali PERSON\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "# Using SpaCy's EntityRuler\n",
    "\n",
    "# Suppose we want to create entity rules Name Entity Rules\n",
    "\n",
    "text = \"Bali was referenced in Mr. Deeds.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a443046-e7c2-4e7f-b4c2-cd3143bf5a7d",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- Bali should reference as GPE instead PERSON. The model fail to recognize it.\n",
    "- Deeds is PERSON, however we should extract it as a whole entity: \"Mr. Deeds\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70bf4c00-2f72-4ba4-a507-4f76c05cbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.pipeline.entityruler.EntityRuler'>\n"
     ]
    }
   ],
   "source": [
    "# Adding entity ruler and return EntityRuler pipeline object\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "print(type(ruler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b642f0-60f8-437f-ad41-df75c5947d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5314554-7f15-4e68-880c-355397624724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adding pattern\n",
    "patterns = [\n",
    "    {'label': 'GPE', 'pattern': 'Bali'}\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2fdbd69-17d0-4696-bdbd-7f8367fd8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "578f2cb7-0467-43d5-85f0-c787d96dcc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bali PERSON\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efddad5-a295-43e3-b6bb-28615b689573",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "- The Spacy pipeline isolated the object attribute that already assigned first by a pipeline.\n",
    "- Since the 'ner' pipeline come first, then 'entity_ruler' can not change ent.label_ object that already assigned with 'ner' pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "092d275c-b50a-483f-9b9f-9b8670bd71cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bali GPE\n",
      "Mr. Deeds FILM\n"
     ]
    }
   ],
   "source": [
    "# Try putting it in previous ner\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add entity_rules pipeline\n",
    "ruler = nlp2.add_pipe('entity_ruler', before='ner')\n",
    "\n",
    "# Create pattern\n",
    "patterns = [\n",
    "    {'label': 'GPE', 'pattern': 'Bali'},\n",
    "    {'label': 'FILM', 'pattern': 'Mr. Deeds'}]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp2(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050b5aa-f830-4383-8caa-f2c237151253",
   "metadata": {},
   "source": [
    "# How to Use the SpaCy Matcher\n",
    "\n",
    "NOTE: It's good approach in case the pattern matching that we want to do is correlated with linguistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fda167f-caed-47a9-a397-8e1d689b2d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.matcher.matcher.Matcher'>\n",
      "Return matches:  [(16571425990740197027, 6, 7)]\n",
      "Access matches data in nlp vocab:  EMAIL_ADDRESS\n",
      "Matches email:  wmattingly@aol.com\n"
     ]
    }
   ],
   "source": [
    "# Basic example\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "print(type(matcher))\n",
    "\n",
    "# Create pattern\n",
    "pattern = [{'LIKE_EMAIL': True}]\n",
    "# In this example, we will search pattern email\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "# NOTE: \n",
    "#  - Matcher.add(<label>, <patterns>)\n",
    "#    <label> Str label that added into nlp vocab\n",
    "#    <pattern> List of list patterns\n",
    "\n",
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
    "matches = matcher(doc)\n",
    "# Return list of tuples that contain (<lexeme>, <start_position>, <end_position>)\n",
    "# NOTE:\n",
    "#  - <lexeme> is special unique index for access matches data in nlp.vocab\n",
    "print(\"Return matches: \", matches)\n",
    "\n",
    "# Access matches label in nlp vocab\n",
    "print(\"Access matches data in nlp vocab: \", nlp.vocab[matches[0][0]].text)\n",
    "\n",
    "# Print location of matches data\n",
    "start_pos = matches[0][1]\n",
    "end_pos = matches[0][2]\n",
    "print(\"Matches email: \", doc[start_pos:end_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5bcec18-f75c-490d-b8ae-cdfa6b0b7fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Luther King Jr. (born Michael King Jr.; January 15, 1929 â€“ April 4, 1968) was an American Baptist minister and activist who became the most visible spokesman and leader in the American civil rights movement from 1955 until his assassination in 1968. King advanced civil rights through nonviolence and civil disobedience, inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi. He was the son of early civil rights activist and minister Martin Luther King Sr.\n",
      "\n",
      "King participated in and led marches for blacks' right to vote, desegregation, labor rights, and other basic civil rights.[1] King led the 1955 Montgomery bus boycott and later became the first president of the Southern Christian Leadership Conference (SCLC). As president of the SCLC, he led the unsuccessful Albany Movement in Albany, Georgia, and helped organize some of the nonviolent 1963 protests in Birmingham, Alabama. King helped organize the 1963 March on Washington, where he delivered his famous \"I Have a Dream\" speech on the steps of the Lincoln Memorial.\n",
      "\n",
      "The SCLC put into practice the tactics of nonviolent protest with some success by strategically choosing the methods and places in which protests were carried out. There were several dramatic stand-offs with segregationist authorities, who sometimes turned violent.[2] Federal Bureau of Investigation (FBI) Director J. Edgar Hoover considered King a radical and made him an object of the FBI's COINTELPRO from 1963, forward. FBI agents investigated him for possible communist ties, recorded his extramarital affairs and reported on them to government officials, and, in 1964, mailed King a threatening anonymous letter, which he interpreted as an attempt to make him commit suicide.[3]\n",
      "\n",
      "On October 14, 1964, King won the Nobel Peace Prize for combating racial inequality through nonviolent resistance. In 1965, he helped organize two of the three Selma to Montgomery marches. In his final years, he expanded his focus to include opposition towards poverty, capitalism, and the Vietnam War.\n",
      "\n",
      "In 1968, King was planning a national occupation of Washington, D.C., to be called the Poor People's Campaign, when he was assassinated on April 4 in Memphis, Tennessee. His death was followed by riots in many U.S. cities. Allegations that James Earl Ray, the man convicted of killing King, had been framed or acted in concert with government agents persisted for decades after the shooting. King was posthumously awarded the Presidential Medal of Freedom in 1977 and the Congressional Gold Medal in 2003. Martin Luther King Jr. Day was established as a holiday in cities and states throughout the United States beginning in 1971; the holiday was enacted at the federal level by legislation signed by President Ronald Reagan in 1986. Hundreds of streets in the U.S. have been renamed in his honor, and the most populous county in Washington State was rededicated for him. The Martin Luther King Jr. Memorial on the National Mall in Washington, D.C., was dedicated in 2011.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Suppose we want to extract all proper nouns\n",
    "\n",
    "with open(\"data_spacy/wiki_mlk.txt\", 'r') as f:\n",
    "   text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "548de431-cf25-4338-8286-f4021206c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Luther King Jr. (born Michael King Jr.; January 15, 1929 â€“ April 4, 1968) was an American Baptist minister and activist who became the most visible spokesman and leader in the American civil rights movement from 1955 until his assassination in 1968. \n",
      "\n",
      "King advanced civil rights through nonviolence and civil disobedience, inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi. \n",
      "\n",
      "He was the son of early civil rights activist and minister Martin Luther King Sr.\n",
      "\n",
      "King participated in and led marches for blacks' right to vote, desegregation, labor rights, and other basic civil rights.[1] \n",
      "\n",
      "King led the 1955 Montgomery bus boycott and later became the first president of the Southern Christian Leadership Conference (SCLC). \n",
      "\n",
      "As president of the SCLC, he led the unsuccessful Albany Movement in Albany, Georgia, and helped organize some of the nonviolent 1963 protests in Birmingham, Alabama. \n",
      "\n",
      "King helped organize the 1963 March on Washington, where he delivered his famous \"I Have a Dream\" speech on the steps of the Lincoln Memorial.\n",
      "\n",
      " \n",
      "\n",
      "The SCLC put into practice the tactics of nonviolent protest with some success by strategically choosing the methods and places in which protests were carried out. \n",
      "\n",
      "There were several dramatic stand-offs with segregationist authorities, who sometimes turned violent.[2] Federal Bureau of Investigation (FBI) Director J. Edgar Hoover considered King a radical and made him an object of the FBI's COINTELPRO from 1963, forward. \n",
      "\n",
      "FBI agents investigated him for possible communist ties, recorded his extramarital affairs and reported on them to government officials, and, in 1964, mailed King a threatening anonymous letter, which he interpreted as an attempt to make him commit suicide.[3]\n",
      "\n",
      "On October 14, 1964, King won the Nobel Peace Prize for combating racial inequality through nonviolent resistance. \n",
      "\n",
      "In 1965, he helped organize two of the three Selma to Montgomery marches. \n",
      "\n",
      "In his final years, he expanded his focus to include opposition towards poverty, capitalism, and the Vietnam War.\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(doc.sents):\n",
    "    print(sent, \"\\n\")\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81a88c9d-f05a-418d-b289-8d2f07a7fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "(451313080118390996, 0, 1) Martin\n",
      "(451313080118390996, 1, 2) Luther\n",
      "(451313080118390996, 2, 3) King\n",
      "(451313080118390996, 3, 4) Jr.\n",
      "(451313080118390996, 6, 7) Michael\n",
      "(451313080118390996, 7, 8) King\n",
      "(451313080118390996, 8, 9) Jr.\n",
      "(451313080118390996, 10, 11) January\n",
      "(451313080118390996, 16, 17) April\n",
      "(451313080118390996, 24, 25) Baptist\n"
     ]
    }
   ],
   "source": [
    "# Load nlp model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'PROPN'}]\n",
    "matcher.add(\"PROPER_NOUN\", [pattern])\n",
    "\n",
    "# Create doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620ff09-57a2-4e93-af26-48a5e8dce4ec",
   "metadata": {},
   "source": [
    "NOTE: The problem is all proper nouns are just individual tokens. We need to fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c63b9779-22d5-4485-8f0b-f63357c0ae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "(451313080118390996, 0, 1) Martin\n",
      "(451313080118390996, 0, 2) Martin Luther\n",
      "(451313080118390996, 1, 2) Luther\n",
      "(451313080118390996, 0, 3) Martin Luther King\n",
      "(451313080118390996, 1, 3) Luther King\n",
      "(451313080118390996, 2, 3) King\n",
      "(451313080118390996, 0, 4) Martin Luther King Jr.\n",
      "(451313080118390996, 1, 4) Luther King Jr.\n",
      "(451313080118390996, 2, 4) King Jr.\n",
      "(451313080118390996, 3, 4) Jr.\n"
     ]
    }
   ],
   "source": [
    "# Create matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'PROPN', 'OP':'+'}] # Extract pattern that match 1 or more times\n",
    "matcher.add(\"PROPER_NOUN\", [pattern])\n",
    "\n",
    "# Create doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce367c-2333-4ea4-93de-3fb240121194",
   "metadata": {},
   "source": [
    "NOTE: It has done exactly what we want to do however, there are some overlaps. It's grabbing all of these in any combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8cb713f2-a4be-4142-9460-fe781c062cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "(451313080118390996, 0, 4) Martin Luther King Jr.\n",
      "(451313080118390996, 6, 9) Michael King Jr.\n",
      "(451313080118390996, 10, 11) January\n",
      "(451313080118390996, 16, 17) April\n",
      "(451313080118390996, 24, 25) Baptist\n",
      "(451313080118390996, 50, 51) King\n",
      "(451313080118390996, 70, 72) Mahatma Gandhi\n",
      "(451313080118390996, 84, 89) Martin Luther King Sr.\n",
      "(451313080118390996, 90, 91) King\n",
      "(451313080118390996, 114, 115) King\n"
     ]
    }
   ],
   "source": [
    "# Create matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'PROPN', 'OP':'+'}] # Extract pattern that match 1 or more times\n",
    "matcher.add(\"PROPER_NOUN\", [pattern], greedy='LONGEST') # Only chose the longset pattern and neglect overlaping\n",
    "\n",
    "# Create doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "# Sort the matches (In the default, LONGEST greedy argument makes the result is not sorted by\n",
    "#  index position as previously\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabaa7a7-6bf1-4d16-8c92-ea094873577e",
   "metadata": {},
   "source": [
    "How does it work?\n",
    "\n",
    "Since pattern = [{'POS': 'PROPN', 'OP':'+'}], it means that token with pattern 'PROPN' will be extracted. When it comes to others then 'PROPN', it will be see as different pattern.\n",
    "\n",
    "For example\n",
    "\n",
    "\"Martin Luther King Jr. (born Michael King Jr.; ...\"\n",
    "\n",
    "Martin = 'PROPN'\n",
    "Luther = 'PROPN'\n",
    "King = 'PROPN'\n",
    "Jr = 'PROPN'\n",
    ". = 'PUNCTUATION' (it will stop in here)\n",
    "( = 'PUNCTUATION'\n",
    "born = 'VERB'\n",
    "Micahel = 'PROPN' (start again in here)\n",
    "King = 'PROPN'\n",
    "Jr = 'PROPN'\n",
    "\n",
    "So on ...\n",
    "\n",
    "Since we do not set the greedy argument as the \"LONGEST\", we will get the combination of PROPN pattern until its end pattern location. If we set greedy argument as \"LONGEST\", then it will filtering out only the longest combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0c07eac-9f00-4c99-9b16-aeafef6bcfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(451313080118390996, 50, 52) King advanced\n",
      "(451313080118390996, 90, 92) King participated\n",
      "(451313080118390996, 114, 116) King led\n",
      "(451313080118390996, 168, 170) King helped\n",
      "(451313080118390996, 248, 253) Director J. Edgar Hoover considered\n",
      "(451313080118390996, 323, 325) King won\n",
      "(451313080118390996, 486, 489) United States beginning\n"
     ]
    }
   ],
   "source": [
    "# Create matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'PROPN', 'OP':'+'}, {\"POS\": \"VERB\"}] \n",
    "# PROPN that match 1 or more times + VERB\n",
    "matcher.add(\"PROPER_NOUN\", [pattern], greedy='LONGEST') # Only chose the longset pattern and neglect overlaping\n",
    "\n",
    "# Create doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "# Sort the matches (In the default, LONGEST greedy argument makes the result is not sorted by\n",
    "#  index position as previously\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b2c29c9-976d-47e7-80ae-ee9baca6e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Suppose we want to extract quotation marks and any instances where there's a quote and somebody\n",
    "#  speaking.\n",
    "import json\n",
    "with open(\"data_spacy/alice.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "text = data[0][2][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c4102ce-9faf-4d69-b1e0-daf85b53f219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\"`\", \"'\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd54c6-11b2-476a-8a50-be6f8f89546d",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "Make sure to standardized text data, things like quotation marks will always throw off our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2fdbaa45-1f83-4561-82d3-3c8ad585363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "speak_lemmas = ['think', 'say']\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'ORTH': \"'\"},\n",
    "           {'IS_ALPHA': True, \"OP\": \"+\"}, # all alpha numeric that matches 1 or more times\n",
    "           {'IS_PUNCT': True, \"OP\": \"*\"}, # all possible punctuation that matches 0 or more (it can be contain or not)\n",
    "           {'ORTH': \"'\"},\n",
    "           {'POS':\"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},\n",
    "           {'POS': 'PROPN', \"OP\": \"+\"},\n",
    "           {'ORTH': \"'\"},\n",
    "           {'IS_ALPHA': True, \"OP\": \"+\"}, # all alpha numeric that matches 1 or more times\n",
    "           {'IS_PUNCT': True, \"OP\": \"*\"}, # all possible punctuation that matches 0 or more (it can be contain or not)\n",
    "           {'ORTH': \"'\"},\n",
    "           ]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b155124-219d-4a23-b1b8-5c60a6d2e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total matches:  1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "2. Total matches:  0\n",
      "3. Total matches:  0\n",
      "4. Total matches:  0\n",
      "5. Total matches:  0\n",
      "6. Total matches:  0\n",
      "7. Total matches:  0\n",
      "8. Total matches:  0\n",
      "9. Total matches:  0\n",
      "10. Total matches:  0\n",
      "11. Total matches:  0\n",
      "12. Total matches:  0\n",
      "13. Total matches:  0\n",
      "14. Total matches:  0\n",
      "15. Total matches:  0\n",
      "16. Total matches:  0\n",
      "17. Total matches:  0\n"
     ]
    }
   ],
   "source": [
    "# Try is it work in general?\n",
    "# print(data[0][2])\n",
    "for idx, text in enumerate(data[0][2]):\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"{idx + 1}. Total matches: \", len(matches))\n",
    "    matches.sort(key= lambda x: x[1])\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b8bd44d-b595-4d0a-93c5-c0f661c10e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 0, 6) 'Well!' thought Alice\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 57, 68) 'which certainly was not here before,' said Alice\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Try add multiple patterns\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace( \"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "pattern2 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print (len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca34daf-ecc9-4a22-8f63-a4c20b26ab97",
   "metadata": {},
   "source": [
    "# Custom Components in SpaCy\n",
    "\n",
    "Custom component: something that changes the doc object along the way in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6704a547-b649-46a6-a258-4e2a3dd08571",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "069dff3f-9bd5-4b75-bb5e-cee1169afb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3eefa318-8e06-4971-8591-12aef63ac73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose that we want to remove all instances of GPE from doc.ents container\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49374db4-d00c-4c61-83d1-66cf78b564e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_gpe(doc)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add to nlp pipeline\n",
    "nlp.add_pipe(\"remove_gpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3ab011b-af12-43ba-a4d4-a015eb63ad54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'remove_gpe': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'remove_gpe': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc7e5e98-413c-4730-b0d2-816077dda7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11d29997-742b-4347-acd6-601919e712b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current model\n",
    "# nlp.to_disk(\"data_spacy/new_en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a7405-e8db-4967-9d62-637c4226bc7d",
   "metadata": {},
   "source": [
    "NOTE: When we use this saved model, the custom component must be exist in the script that the saved model is loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b95b9-75e9-4e6d-86a1-d2effc932b7b",
   "metadata": {},
   "source": [
    "# Regex Multi Word Tokens\n",
    "\n",
    "How to use Regex in Named Entity Recognition:\n",
    "1. Find matches pattern with Regex\n",
    "2. Create new Span object of matched pattern.\n",
    "3. Inject new Span into the doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ee0abc6-ac83-43a2-a6c6-16cb40f7fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "946dc5c5-3ca0-463a-b9a5-5c81595bb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc80f975-d379-41de-a14e-b9c6ac36413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'callable_iterator'>\n",
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "The index location matches in text:  (0, 11)\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n",
      "The index location matches in text:  (39, 53)\n"
     ]
    }
   ],
   "source": [
    "# Analyze pattern\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "# NOTE: There are many \"Paul\" name in this text. We want to give them name based on its context.\n",
    "\n",
    "# Extract pattern\n",
    "pattern = r'Paul [A-Z]\\w+'\n",
    "\n",
    "matches = re.finditer(pattern, text) # Return iterator. Each element is re.Match object\n",
    "print(type(matches)) \n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    print(\"The index location matches in text: \", match.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7538b688-838e-4d67-b129-c8fc8adfeaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.char_span(0, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3112dd2b-c47c-497b-9b6d-e08f5af472de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "None\n",
      "[(0, 2, 'Paul Newman'), (8, 10, 'Paul Hollywood')]\n",
      "After: \n",
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "    \n",
    "# Define doc object\n",
    "doc = nlp(text)\n",
    "original_ents = list(doc.ents)\n",
    "\n",
    "# Define pattern\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "\n",
    "print(\"Before: \")\n",
    "if len(doc.ents) > 0:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "else:\n",
    "    print(\"None\")\n",
    "\n",
    "# Define storage\n",
    "mwt_ents = []\n",
    "# 1. Find matches pattern\n",
    "for match in re.finditer(pattern, text):\n",
    "    # Extract location based on its text\n",
    "    start, end = match.span()\n",
    "    # 2. Create new Span object from the text\n",
    "    span = doc.char_span(start, end) # char_span(start, end) returns Span object based on text position (not token position)\n",
    "    if span is not None:\n",
    "        # Store the span text, span start and end position (token based position)\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "print(mwt_ents)\n",
    "\n",
    "# 3. Inject new Span into the doc.ents\n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "    original_ents.append(per_ent)\n",
    "\n",
    "doc.ents = original_ents\n",
    "\n",
    "print(\"After: \")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "71cde83d-724a-45a9-8c0a-8d1cf53e5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all of this into custom components\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "@Language.component(\"paul_ner\")\n",
    "def paul_ner(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label='PERSON')\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d3314a81-1c40-46a6-b84c-7fbdcb6dab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.paul_ner(doc)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2 = spacy.blank('en')\n",
    "nlp2.add_pipe(\"paul_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b9eedec-d0db-4c97-915b-0514fff8f767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'paul_ner': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'paul_ner': []},\n",
       " 'attrs': {}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ae0f416a-0e1f-4fcd-b3dc-cf4ed3d7802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Paul Newman, Paul Hollywood)\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp2(text)\n",
    "print(doc2.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dca909-bfdf-4812-9c3b-2d678316e8c3",
   "metadata": {},
   "source": [
    "NOTE: Overlap Span object can break the model and turns it into error. \n",
    "\n",
    "For example suppose we have two Span (1) \"Paul Hollywood\" and (2) \"Hollywood\" with overlaping in their position. If we inject those Span into doc.ents it will produce error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "031a227c-9334-4a48-aed3-ac724fe26f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the error from this example\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "pattern = r\"Hollywood\"\n",
    "@Language.component(\"cinema_ner\")\n",
    "def cinema_ner(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label='PERSON')\n",
    "        original_ents.append(per_ent)\n",
    "    # NOTE: Comment below line to see the error as mentioned in above cell.\n",
    "    original_ents = filter_spans(original_ents)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ef0bb9c5-2171-4d57-94cb-d57f795a79d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cinema_ner(doc)>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3 = spacy.load('en_core_web_sm')\n",
    "nlp3.add_pipe(\"cinema_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1659c3a8-01ff-40fd-95e4-294bd2bb21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Paul Newman, American, Paul Hollywood, British, Paul)\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp3(text)\n",
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6e8b8-26cd-48b6-b41f-b87780ee7001",
   "metadata": {},
   "source": [
    "NOTE: Using filter_spans method helps to filter out all the spans that already identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b8b22-cbb5-4404-bff2-aabba0bd2ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
