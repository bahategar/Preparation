{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3db6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4ec1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all', halt_on_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4573e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the corpus:  157171\n",
      "First 100 characters in the corpus\n",
      "  \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "print(\"Total characters in the corpus: \", len(german_text))\n",
    "print(\"First 100 characters in the corpus\\n\", german_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0807c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall NLTK\n",
    "# import os\n",
    "# import shutil\n",
    "# import nltk\n",
    "\n",
    "# # Create the path\n",
    "# # nltk_data_path = os.path.join(user_home, users, username, \n",
    "# #                               appdata, roaming, nltk_data)\n",
    "# nltk_data_path = r'C:\\Users\\Baha Tegar\\AppData\\Roaming\\nltk_data'\n",
    "\n",
    "\n",
    "# # Check if the directory exists\n",
    "# if os.path.exists(nltk_data_path):\n",
    "#     # Remove the directory and its contents\n",
    "#     shutil.rmtree(nltk_data_path)\n",
    "#     print(f\"Removed nltk_data directory at {nltk_data_path}\")\n",
    "# else:\n",
    "#     print(\"nltk_data directory does not exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c716eca",
   "metadata": {},
   "source": [
    "# Parts of Speech (POS) Tagging\n",
    "\n",
    "> POS tagging is giving label into words.\n",
    "\n",
    "> Parts of Speech are specific lexical categories to which words are assigned based on their syntactic context and role.\n",
    "\n",
    "\n",
    "NOTE:  \n",
    "> The origin of the word ‘lexical’ is believed to be the Greek word ‘lexis’ which somewhat means ‘vocabulary’; the total stack of words found in a certain language.\n",
    "\n",
    "> Anything ‘lexical’ in 21st Century is believed to be somewhere related to a study attempting to find a stable relation and coordination between the words used in a language.\n",
    "\n",
    "> The term \"lexical meaning\" refers to the inherent meaning of a word or lexical unit as it is understood in a language, independent of the context in which it is used.\n",
    "\n",
    "- Lexical units includes words, phrases, idioms, and expressions that have specific meanings in a language.\n",
    "\n",
    "Example:\n",
    "\n",
    "Word: \"Apple\"\n",
    "\n",
    "- Denotation: A round fruit with red, green, or yellow skin and a whitish interior.\n",
    "- Connotation: Health, knowledge (as in the story of Adam and Eve), and technology (Apple Inc.).\n",
    "\n",
    "Word: \"Snake\"\n",
    "\n",
    "- Denotation: A long, limbless reptile with a scaly skin.\n",
    "- Connotation: Danger, deceit (as in calling someone a \"snake\"), and treachery.\n",
    "\n",
    "Idiom: \"Break the ice\"\n",
    "\n",
    "- Denotation: To do something to initiate conversation in a social setting.\n",
    "- Connotation: Starting a conversation or activity to ease tension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7e3d0",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- We will use Penn Treebank. Read more: https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/Penn-Treebank-Tagset.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fadacd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n",
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "print(sentence)\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2560fd8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Using pattern module\n",
    "from pattern.en import tag\n",
    "try:\n",
    "    tagged_sent = tag(sentence)\n",
    "except:\n",
    "    StopIteration\n",
    "    \n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd1bcc",
   "metadata": {},
   "source": [
    "# Shallow Parsing\n",
    "\n",
    "The algorithm:\n",
    "- POS Tagging\n",
    "> Break sentence into smallest token/word and tag it with POS.\n",
    "\n",
    "- Chunks identified\n",
    "> The parsing identifies contiguous or non-contiguous sequences of words that form meaningful units, such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), etc.\n",
    "\n",
    "- Group them together\n",
    "> Group them together into higher-level phrases and identified it until reach the initial sentence-level.\n",
    "\n",
    "## Limitations:\n",
    "- Lack of Deep Syntactic Structure: Shallow parsing does not provide detailed syntactic relationships or hierarchical structures.\n",
    "\n",
    "- Ambiguity Handling: It may struggle with resolving syntactic ambiguities that require deeper context understanding.\n",
    "\n",
    "- Context Sensitivity: Shallow parsing relies heavily on surface patterns and may not capture nuanced grammatical or semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2f3f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence('The/DT/B-NP/O brown/JJ/I-NP/O fox/NN/I-NP/O is/VBZ/B-VP/O quick/JJ/B-ADJP/O and/CC/O/O he/PRP/B-NP/O is/VBZ/B-VP/O jumping/VBG/I-VP/O over/IN/B-PP/O the/DT/O/O lazy/JJ/B-ADJP/O dong/IN/B-PP/O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pattern.en import parsetree\n",
    "\n",
    "try:\n",
    "    sentence = 'The brown fox is quick and he is jumping over the lazy dong'\n",
    "\n",
    "    tree = parsetree(sentence)\n",
    "except:\n",
    "    StopIteration\n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af7feb",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- The I prefix => It is inside a chunk.\n",
    "- The B prefix => It is beginning of a chunk.\n",
    "- The O prefix => It does not belong to any chunk.\n",
    "\n",
    "The B- tag is always used when there are subsequent tags following it of the same type without the presence of O tags between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[0].constituents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51751f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all chunks\n",
    "print(\"All chunks:\")\n",
    "for sentence_tree in tree:\n",
    "    print(sentence_tree.chunks)\n",
    "\n",
    "print()\n",
    "# Depict each phrase and its internal constituents\n",
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print(chunk.type, '->', [(word.string, word.type) for word in chunk.words])\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a1397",
   "metadata": {},
   "source": [
    "**Generic Functions Shallow Parse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64283340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some generic functions to parse and visualized shallow parsed sentence trees\n",
    "\n",
    "from pattern.en import parsetree, Chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Create a shallow parsed sentence tree\n",
    "def create_sentence_tree(sentence, lemmatize=False):\n",
    "    sentence_tree = parsetree(sentence,\n",
    "                              relations=True,\n",
    "                              lemmata=lemmatize)\n",
    "    return sentence_tree[0]\n",
    "\n",
    "# Get various constituents of the parse tree\n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "\n",
    "# Process the shallow aprsed tree into an easy to understand format\n",
    "def process_sentence_tree(sentence_tree):\n",
    "    \n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "        (item.type, [(w.string, w.type) for w in item.words]) \n",
    "            if type(item) == Chunk\n",
    "            else (\"-\", [(item.string, item.type)])\n",
    "        for item in tree_constituents\n",
    "    ]\n",
    "    return processed_tree\n",
    "\n",
    "# Print the sentence tree using nltk's Tree syntax\n",
    "def print_sentence_tree(sentence_tree):\n",
    "    \n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0], [Tree(x[1], [x[0]]) for x in item[1]]) for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S', processed_tree)\n",
    "    print(tree)\n",
    "    \n",
    "# Visualize the sentence tree using nltk's Tree syntax\n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0], [Tree(x[1], [x[0]]) for x in item[1]])\n",
    "            for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S', processed_tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3930aa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence('The/DT/B-NP/O/NP-SBJ-1 brown/JJ/I-NP/O/NP-SBJ-1 fox/NN/I-NP/O/NP-SBJ-1 is/VBZ/B-VP/O/VP-1 quick/JJ/B-ADJP/O/O and/CC/O/O/O he/PRP/B-NP/O/NP-SBJ-2 is/VBZ/B-VP/O/VP-2 jumping/VBG/I-VP/O/VP-2 over/IN/B-PP/O/O the/DT/O/O/O lazy/JJ/B-ADJP/O/O dong/IN/B-PP/O/O')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw shallow parsed tree\n",
    "t = create_sentence_tree(sentence)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2025d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NP', [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]),\n",
       " ('VP', [('is', 'VBZ')]),\n",
       " ('ADJP', [('quick', 'JJ')]),\n",
       " ('-', [('and', 'CC')]),\n",
       " ('NP', [('he', 'PRP')]),\n",
       " ('VP', [('is', 'VBZ'), ('jumping', 'VBG')]),\n",
       " ('PP', [('over', 'IN')]),\n",
       " ('-', [('the', 'DT')]),\n",
       " ('ADJP', [('lazy', 'JJ')]),\n",
       " ('PP', [('dong', 'IN')])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed shallow parsed tree\n",
    "pt = process_sentence_tree(t)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e1d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (- (DT the))\n",
      "  (ADJP (JJ lazy))\n",
      "  (PP (IN dong)))\n"
     ]
    }
   ],
   "source": [
    "# print shallow parsed tree in an easy to understand format \n",
    "# using nltk's Tree syntax\n",
    "\n",
    "print_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5feebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the shallow parsed tree\n",
    "visualize_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d8044",
   "metadata": {},
   "source": [
    "# Dependency-based Parsing\n",
    "\n",
    "> Using dependency-based grammars to analyze and infer both structure and semantic dependencies and relationships between tokens in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68f8d3",
   "metadata": {},
   "source": [
    "**Using spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bec2a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "# # Load dependencies\n",
    "# import spacy\n",
    "# parser = spacy.load('en_core_web_sm')\n",
    "# parsed_sent = parser(sentence)\n",
    "\n",
    "# # Generate dependency parser output\n",
    "# dependency_pattern = '{left} <--- {word}[{w_type}] ---> {right}\\n------'\n",
    "# for token in parsed_sent:\n",
    "#     print(dependency_pattern.format(word=token.orth_,\n",
    "#                                     w_type=token.dep_,\n",
    "#                                     left=[t.orth_ for t in token.lefts],\n",
    "#                                     right=[t.orth_ for t in token.rights]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14fd3e",
   "metadata": {},
   "source": [
    "**Using Standford Dependency Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbf91c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.parse.stanford import StanfordDependencyParser\n",
    "# import os\n",
    "\n",
    "# sdp = StanfordDependencyParser(path_to_jar=r'C:\\Users\\Baha Tegar\\Desktop\\Preparation\\TextAnalytics\\stanford-parser-4.2.0\\stanford-parser-full-2020-11-17\\stanford-parser.jar',\n",
    "#                                path_to_models_jar=r'C:\\Users\\Baha Tegar\\Desktop\\Preparation\\TextAnalytics\\stanford-parser-4.2.0\\stanford-parser-full-2020-11-17\\stanford-parser-4.2.0-models.jar')\n",
    "\n",
    "# result = list(sdp.raw_parse(sentence))\n",
    "\n",
    "# result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b2e4b",
   "metadata": {},
   "source": [
    "# Constituency-based Parsing\n",
    "\n",
    ">  It involves analyzing the syntactic structure of a sentence by breaking it down into its constituent parts or phrases. Each constituent is a group of words that functions as a single unit within a hierarchical structure, typically represented as a tree.\n",
    "\n",
    "There are various types of parsing algorithms, including the following:\n",
    "- Recursive Descent parsing\n",
    "- Shift Reduce parsing\n",
    "- Chart parsing\n",
    "- Bottom-up parsing\n",
    "- Top-down parsing\n",
    "- PCFG parsing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_kernel",
   "language": "python",
   "name": "text_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
