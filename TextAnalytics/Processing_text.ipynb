{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ae11591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275359a",
   "metadata": {},
   "source": [
    "# Text Tokenization\n",
    "- Tokens => Independent and minimal textual components that have some definite syntax and semantics.\n",
    "\n",
    "NOTE: This minimal textual component can be sentences, clauses, phrases, or words depending on what you want.\n",
    "\n",
    "- Tokenization => The process of breaking down or splitting textual data into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675e83b",
   "metadata": {},
   "source": [
    "## A. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85ba6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy an\n"
     ]
    }
   ],
   "source": [
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "\n",
    "print(alice[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25aab88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python codewhich you should remember when writing code! Python is a really powerful programming language!\n"
     ]
    }
   ],
   "source": [
    "sample_text = '''We will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python codewhich you should remember when writing code! Python is a really powerful programming language!'''\n",
    "\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf4098",
   "metadata": {},
   "source": [
    "**The nltk.sent_tokenize Function**\n",
    "\n",
    "> The nltk.sent_tokenize function is the default sentence tokenization function that nltk recommends. \n",
    "\n",
    "> It uses an instance of the PunktSentenceTokenizer class internally. However, this is not just a normal object or instance of that class—it has been pre-trained on several language models and works really well on many popular languages besides just English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8d7aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nltk.sent_tokenize funtion\n",
    "from nltk import sent_tokenize\n",
    "# or\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "alice_sentences = sent_tokenize(text=alice)\n",
    "sample_sentences = sent_tokenize(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5812ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 3\n",
      "Sample text sentences :\n",
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python codewhich you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :')\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66dbae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:')\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74982c2d",
   "metadata": {},
   "source": [
    "**Using pre-trained tokenization model into PunkSentenceTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9525170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the corpus:  157171\n",
      "First 100 characters in the corpus\n",
      "  \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "print(\"Total characters in the corpus: \", len(german_text))\n",
    "print(\"First 100 characters in the corpus\\n\", german_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54f0ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .', 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .', 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .', 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .', 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
     ]
    }
   ],
   "source": [
    "# Using pre-trained tokenization\n",
    "\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "print(german_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80c43972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .', 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .', 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .', 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .', 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
     ]
    }
   ],
   "source": [
    "# Note: We can use sent_tokenizer too\n",
    "\n",
    "german_sentences_st = sent_tokenize(text=german_text, language='german')\n",
    "\n",
    "print(german_sentences_st[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93f69117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the result is same\n",
    "\n",
    "german_sentences_st == german_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d237f8",
   "metadata": {},
   "source": [
    "**Using RegexpTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "122ee18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python codewhich you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regex_st = RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,\n",
    "                           gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d74b2",
   "metadata": {},
   "source": [
    "## B. Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993a52a",
   "metadata": {},
   "source": [
    "**Using nltk.word_tokenize Function**\n",
    "\n",
    "> It is the default and recommended work tokenizer as specified by nltk.\n",
    "\n",
    "> This tokenizer is actually an instance or object of the TreebankWordTokenizer class in its internal implementation and acts as a warpper to that core class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87072556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.']\n"
     ]
    }
   ],
   "source": [
    "# Using nltk.word_tokenize function\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race.\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba4af3",
   "metadata": {},
   "source": [
    "**Using TreebankWordTokenizer**\n",
    "\n",
    "> It's based on the Penn Treebank and uses various regular expressions to tokenize the text.\n",
    "\n",
    "Read more about Penn Treebank: www.cis.upenn.edu/~treebank/tokenizer.sed\n",
    "\n",
    "The main features of this tokenizer:\n",
    "- Splits and separates out periods that appear at the end of a sentence.\n",
    "- Splits and separates commas and single quotes when followed by whitespace.\n",
    "- Most punctuation characters are split and separated into independent tokens.\n",
    "- Splits words with standard contractions. Examples would be don't to do and n't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d89d96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf50f1d",
   "metadata": {},
   "source": [
    "NOTE: Since word_tokenize function and TreebankWordTokenizer using same tokenizing mechanism, then the outputs are same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457de6e",
   "metadata": {},
   "source": [
    "**Using RegexpTokenizer**\n",
    "\n",
    "There are two main parameters:\n",
    "- pattern ==> For building the tokenizer.\n",
    "- gaps ==> If set to True then it will find the gaps between the tokens. Otherwise it is used to find the tokens themselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fa017b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# Pattern to identify tokens themselves\n",
    "\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,\n",
    "                                gaps=False)\n",
    "\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a25a666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race.']\n"
     ]
    }
   ],
   "source": [
    "# Pattern to identify gaps in tokens\n",
    "\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
    "                                gaps=True)\n",
    "\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666028cd",
   "metadata": {},
   "source": [
    "**WordPunctTokenizer & WhiteSpaceTokenizer**\n",
    "> The WordPunctTokenizer uses the pattern r'\\w+|[^\\w\\s]+' to tokenize sentences into independent alphabetic and non-alphabetic tokens.\n",
    "\n",
    "> The WhiteSpaceTokenizer uses whitespaces (tabs, newlines, and spaces) to tokenizes sentences into word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90c33888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "\n",
    "wordpunk_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunk_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98285837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race.']\n"
     ]
    }
   ],
   "source": [
    "# WhiteSpaceTokenizer\n",
    "\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b907c",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "> Process wrangle, clean, and standardize textual data into form that could be consumed by other NLP and analytics systems and applications as input.\n",
    "\n",
    "NOTE: \n",
    "- Text normalization also called text cleansing or wrangling.\n",
    "- Tokenization is a part of text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1625dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for $199\", \"@@You'll (learn) a **lot** in the book. Python is an amazing language !@@\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language !@@\"]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180f8e7",
   "metadata": {},
   "source": [
    "**Cleaning Text**\n",
    "\n",
    "> Often the textual data we want to use or analyze contains a lot of extraneous and unnecessary tokens and characters that should be removed before performing any further operations like tokenization or other normalization techniques.\n",
    "\n",
    "For example:\n",
    "- Extract meaningful text from HTML => We can use (1) clean_html() function from nltk, (2) Parse HTML data using BeautifulSoup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c150671",
   "metadata": {},
   "source": [
    "**Tokenizing Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "009c4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']], [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'], ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']], [['@', '@', 'You', \"'ll\", '(', 'learn', ')', 'a', '*', '*', 'lot', '*', '*', 'in', 'the', 'book', '.'], ['Python', 'is', 'an', 'amazing', 'language', '!'], ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b24f0a",
   "metadata": {},
   "source": [
    "**Removing Special Characters**\n",
    "\n",
    "- These may be special symbols or even punctuation that occurs in sentences.\n",
    "\n",
    "- The main reason is often punctuation or special characters do not have much significance when we analyze the text and utilize it for extracting features or information based on NLP and ML.\n",
    "\n",
    "- This step is often performed before or after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "baf0b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race']], [['Hey', 'that', 's', 'a', 'great', 'deal'], ['I', 'just', 'bought', 'a', 'phone', 'for', '199']], [['You', 'll', 'learn', 'a', 'lot', 'in', 'the', 'book'], ['Python', 'is', 'an', 'amazing', 'language']]]\n"
     ]
    }
   ],
   "source": [
    "# Example removing special characters after tokenization\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = list(filter(None, [pattern.sub('', token) for token in tokens]))\n",
    "    # filter(None, ...) is used to remove any empty strings that may \n",
    "    #   result from removing all punctuation from a token.\n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_list_1 = [list(filter(None, [remove_characters_after_tokenization(tokens) for tokens in sentence_tokens])) for sentence_tokens in token_list]\n",
    "\n",
    "print(filtered_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a5c6695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not keep some apostrophes:  ['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language ']\n",
      "\n",
      "Keep some apostrophes:  [\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language !\"]\n"
     ]
    }
   ],
   "source": [
    "# Example removing special characters before tokenization\n",
    "\n",
    "def remove_characters_before_tokenization(sentence,\n",
    "                                          keep_apostrophes=False):\n",
    "    \n",
    "    # Removing leading and trailing whitespace from the sentence\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # If keep_apostrophes is True, remove only specific special characters\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        # If keep_apostrophes is False, remove all non-alphanumeric characters except spaces\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "\n",
    "    # Return the cleaned sentence\n",
    "    return filtered_sentence\n",
    "\n",
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, True) for sentence in corpus]\n",
    "\n",
    "print(\"Do not keep some apostrophes: \", filtered_list_2)\n",
    "print()\n",
    "print(\"Keep some apostrophes: \", cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49697f26",
   "metadata": {},
   "source": [
    "**Expanding Contractions**\n",
    "\n",
    "Contractions are shortened version of words or syllables.\n",
    "\n",
    "Why contractions do pose a problem for NLP and text analytics:\n",
    "- Constractions contains a special apostrophe character.\n",
    "- We could have two or more words represented by a contraction (e.g: you'll => you will or you shall). This opens a whole new can of worms when we try to tokenize this or even standardize the words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "497ef702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing language !']\n"
     ]
    }
   ],
   "source": [
    "# We create contractions.py file that contains CONTRACTION MAP.\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                     flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        \n",
    "#         print(\"Contraction\", contraction)\n",
    "#         print(\"First_char\", first_char)\n",
    "#         print(\"Expanded_contraction\", expanded_contraction)\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "#     print(expanded_sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP)\n",
    "                   for sentence in cleaned_corpus]\n",
    "print(expanded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756797a",
   "metadata": {},
   "source": [
    "**Case Conversions**\n",
    "\n",
    "Usually there are two type of case conversion:(1) Lowercase and (2) Uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f4d76bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "print(corpus[0].lower())\n",
    "# Uppercase\n",
    "print(corpus[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e3e93",
   "metadata": {},
   "source": [
    "**Correcting Words**\n",
    "\n",
    "> the WordNet corpus help to validate word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e488c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'schooool', 'is', 'realllllyyy', 'amaaazingggg']\n",
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# Correcting Repeating Characters\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "old_word = 'finalllyyy'\n",
    "step = 1\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "# In format common (not recursive)\n",
    "# repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "# match_substitution = r'\\1\\2\\3'\n",
    "# while True:\n",
    "#     # check for semantically correct word\n",
    "#     if wordnet.synsets(old_word):\n",
    "#         print (\"Final correct word:\", old_word)\n",
    "#         break\n",
    "#     # remove one repeated character\n",
    "#     new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "#     if new_word != old_word:\n",
    "#         print('Step: {} Word: {}'.format(step, new_word))\n",
    "#         step += 1 # update step\n",
    "#         # update old word to last substituted state\n",
    "#         old_word = new_word\n",
    "#         continue\n",
    "#     else:\n",
    "#         print(\"Final word:\", new_word)\n",
    "#         break\n",
    "\n",
    "\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "print(sample_sentence_tokens)\n",
    "print(remove_repeated_characters(sample_sentence_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc5446",
   "metadata": {},
   "source": [
    "**Correcting Spellings**\n",
    "\n",
    "Some algorithms:\n",
    "- Peter Norvig Algorithm (https://norvig.com/spell-correct.html)\n",
    "- PyEnchant (http://pythonhosted.org/pyenchant/)\n",
    "- aspell-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d31ff2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spelling\n",
      "korrectud\n",
      "bycycle\n",
      "fianlly\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "corpus = \"\"\"\n",
    "This is a sample corpus of text. It contains a few sentences with words.\n",
    "This is useful for demonstration purposes, but in practice, you would\n",
    "want a much larger corpus to improve the accuracy of the spelling correction.\n",
    "\"\"\"\n",
    "\n",
    "WORDS = Counter(words(corpus))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "print(correction(\"speling\"))  # Should return \"spelling\"\n",
    "print(correction(\"korrectud\"))  # Should return \"corrected\"\n",
    "print(correction(\"bycycle\"))  # Should return \"bicycle\"\n",
    "print(correction('fianlly')) # Should return \"finally\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8285ae",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "- Stem => Base form of a word.\n",
    "- Inflection => Attaching affixes to base.\n",
    "- Stemming => Separate affixes and its base.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"jumping\" ==> base: \"jump\" and affixes: \"-ing\"\n",
    "\n",
    "\n",
    "Some stemmers:\n",
    "- Porter Stemmer ==> Invented by Dr. Martin Porter\n",
    "> The algorithm is said to have had a total of five different phases for reduction of inflections to their stems, where each phase has its own set of rules.\n",
    "\n",
    "- Porter 2 Stemmer ==> Improvements that suggested by Dr. Porter\n",
    "\n",
    "- Lancaster Stemmer (or Paice/Husk Stemmer) ==> Invented by Chris D. Paice\n",
    "> It is an iterative stemmer that has over 120 rules specifying specific removal or replacement for affixes to obtain the word stems.\n",
    "\n",
    "- RegexpStemmer\n",
    "> Using regular expressions to identify the morphological affixes in words, and any part of the string matching the same is removed.\n",
    "\n",
    "- SnowBallStemmer ==> Supports stemming in 13 different languages besides English (https://snowballstem.org/)\n",
    "\n",
    "\n",
    "NOTE: The Porter Stemmer is used most frequently, but we should choose our stemmer based on our problem and after trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f338390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
    "print(ps.stem('lying'))\n",
    "print(ps.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "08a709df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lying\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "# Lancaster Stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
    "print(ls.stem('lying'))\n",
    "print(ls.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d9224668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "ly\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "# Regex based\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "\n",
    "print(rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
    "print(rs.stem('lying'))\n",
    "print(rs.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be4b0c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "\n",
      "autobahn\n",
      "spring\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "print('Supported Languages:', SnowballStemmer.languages)\n",
    "print()\n",
    "\n",
    "# Set as german\n",
    "\n",
    "ss = SnowballStemmer(\"german\")\n",
    "\n",
    "# stemming on German words\n",
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "print(ss.stem('autobahnen'))\n",
    "\n",
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "print(ss.stem('springen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed3112",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "> The process of lemmatization is very similar to stemming—you remove word affixes to get to a base form of the word. But in this case, this base form is also known as the root word , but not the root stem.\n",
    "\n",
    "\n",
    "NOTE:\n",
    "The difference between root stem & root word:\n",
    "- The root stem may not always be a lexicographically correct word, it may not be present in the dictionary.\n",
    "- The root word (also known as the lemma) will always be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "23c51db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n",
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))\n",
    "\n",
    "# Make sure the part of speech is not wrong\n",
    "print(wnl.lemmatize('ate', 'n')) # it should verb\n",
    "print(wnl.lemmatize('fancier', 'v')) # It should adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819c48",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "\n",
    "> The preceding code leverages the WordNetLemmatizer class, it's internally uses the morphy() function belongin to the WordNetCorpusReader class. \n",
    "\n",
    "> It's basically finds the base form or lemma for a given word using the word and its part of speech by checking the Wordnet corpus and uses a recursive technique for removing affixes from the word until a match is found in WordNet. If no match is found, the input word itself is returned unchanged.\n",
    "\n",
    "> The part of speech is extremely important here because if that is wrong, the lemmatization will not be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd4543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
