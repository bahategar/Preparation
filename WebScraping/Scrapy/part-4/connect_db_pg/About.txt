The goal: Connect scrapy into database and save result.

Docs: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
Another references:
 connect with postgresql: https://thepythonscrapyplaybook.com/scrapy-save-data-postgres/
 connect with mysql: https://thepythonscrapyplaybook.com/scrapy-save-data-mysql/


Save scraping result:
- By setting the default feeder:
  1. Go to settings.py
  2. Write code:
	    a. For csv file:
          FEEDS = {
              'booksdata.csv': {
                  'format': 'csv',
                  'encoding': 'utf8',
                  'store_empty': False,
                  'fields': None,
                  'overwrite': True,
              },
          }

      b. For json file:
          FEEDS = {
              'booksdata.json': {
                  'format': 'json',
                  'encoding': 'utf8',
                  'store_empty': False,
                  'fields': None,
                  'indent': 4,
              },
          }

  NOTE: If we already set the default for feeder, we do not have to write additional command "-O <filename with its extension>"

- Alternative by setting its custom_settings inside the spiders (<spider_name>.py). 
  1. Write new code inside the spider class (defined by scrapy):
      custom_settings = { FEEDS : {
                            'booksdata.json': {
                                    'format': 'json',
                                    'encoding': 'utf8',
                                    'store_empty': False,
                                    'fields': None,
                                    'indent': 4,
                            },
                          }
                        }

    NOTE: It will overwrite the default setting. Also we do not have to write additional command "-O <filename with its extension>"
