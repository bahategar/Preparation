1. Basic pipelines.py file:   
	> Purpose: Allow to perform operations (post-processing) on the scraped data, such as cleaning, validating, saving to databases, or exporting to files.
	> Components (must): Inside a pipeline class, must have "process_item" method.
	> Reference Docs: https://docs.scrapy.org/en/latest/topics/item-pipeline.html

2. Implementation (basic):

   - Define Pipeline class:

     # Inside pipeline.py file

     # useful for handling different item types with a single interface
     # It is not depend on the item's specific type (it for general item's type).

     from itemadapter import ItemAdapter

     # How ItemAdapter Helps:
     # - Uniform Access: You can access and set fields using adapter.get('field_name') or adapter['field_name'] without worrying about the item’s type (dictionary or Item object).
     # - Flexible Field Modification: If the item is a custom class or a dictionary, ItemAdapter provides a unified way to add or modify fields.
     # - Compatibility: ItemAdapter makes it easier to handle complex or custom item structures without needing conditional checks for each type.


     # 1. Define a pipeline class
     class <Pipeline name>:

       
       # 2. Define process_item method, This method is a required method for a Scrapy pipeline.
       def process_item(self, item, spider):
         # NOTE:
	 #   - The "item" parameter represents the current item being processed.
         #   - The "spider" parameter is a reference to the spider that scraped the item, allowing access to spider-specific properties if needed.

	 ## SOME CODES ##
         
         # Define the ItemAdapter object (optional)
         adapter = ItemAdapter(item)
         # NOTE: It wraps around the item object that provides a consistent interface to access, set, and modify item fields, regardless of the item’s type.

	 ## SOME CODES ##

         return item

   - Enable the Pipeline in settings.py
     On the settings.py => configure item pipelines option => Uncomment "ITEM_PIPELINES" variable.

     # Configure item pipelines
     # See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
     ITEM_PIPELINES = {
       "<project name>.pipelines.<Pipeline name>": 300,
      }
     # NOTE: The value of dictionary (integer) represent the priority order. Lower numbers have higher priority, meaning they are executed earlier in the pipeline chain.


3. Create Pipeline for Connecting to Database
   - Code Pipeline Save Scraped Items Into Database with PostreSQL --> requirements: psycopg2 (or similar like that)
     
     # pipelines.py

     import psycopg2

     class PostgresDemoPipeline:

        def __init__(self):
            ## Connection Details
            hostname = 'localhost'
            username = 'postgres'
            password = '******' # your password
            database = 'YOUR DATABASE NAME'

            ## Create/Connect to database
            self.connection = psycopg2.connect(host=hostname, user=username, password=password, dbname=database)
        
            ## Create cursor, used to execute commands
            self.cur = self.connection.cursor()
        
            ## Create quotes table if none exists
            self.cur.execute("""
            CREATE TABLE IF NOT EXISTS quotes(
                id serial PRIMARY KEY, 
                content text,
                tags text,
                author VARCHAR(255)
            )
            """)

      def process_item(self, item, spider):

          ## Define insert statement
          self.cur.execute(""" insert into quotes (content, tags, author) values (%s,%s,%s)""", (
	      item["text"],
              str(item["tags"]),
              item["author"]
          ))

          ## Execute insert of data into database
          self.connection.commit()
          return item

     def close_spider(self, spider):
     
         ## Close cursor & connection to database 
         self.cur.close()
         self.connection.close()


   - Code Pipeline Only Saving New Data with PostgreSQL

     # pipelines.py

     import psycopg2

     class PostgresDemoPipeline:

        def __init__(self):
            ## Connection Details
            hostname = 'localhost'
            username = 'postgres'
            password = '******' # your password
            database = 'YOUR DATABASE NAME'

            ## Create/Connect to database
            self.connection = psycopg2.connect(host=hostname, user=username, password=password, dbname=database)
        
            ## Create cursor, used to execute commands
            self.cur = self.connection.cursor()
        
            ## Create quotes table if none exists
            self.cur.execute("""
            CREATE TABLE IF NOT EXISTS quotes(
                id serial PRIMARY KEY, 
                content text,
                tags text,
                author VARCHAR(255)
            )
            """)

       def process_item(self, item, spider):
           ## Check to see if text is already in database 
           self.cur.execute("select * from quotes where content = %s", (item['text'],))
           result = self.cur.fetchone()

           ## If it is in DB, create log message
           if result:
               spider.logger.warn("Item already in database: %s" % item['text'])


           ## If text isn't in the DB, insert data
           else:

               ## Define insert statement
               self.cur.execute(""" insert into quotes (content, tags, author) values (%s,%s,%s)""", (
                   item["text"],
                   str(item["tags"]),
                   item["author"]
               ))

              ## Execute insert of data into database
              self.connection.commit()
           
           return item

     def close_spider(self, spider):
     
         ## Close cursor & connection to database 
         self.cur.close()
         self.connection.close()


   - Code Pipeline Save Scraped Items Into Database with MySQL --> requirements: mysql (or similar like that)

     # pipelines.py

     import mysql.connector

     class MysqlDemoPipeline:

	
        def __init__(self):
	    ## Create/Connect to database
            self.conn = mysql.connector.connect(
               host = 'localhost',
               user = 'root',
               password = '**********',
               database = 'YOUR DATABASE NAME'
            )

            ## Create cursor, used to execute commands
            self.cur = self.conn.cursor()
        
            ## Create quotes table if none exists
            self.cur.execute("""
            CREATE TABLE IF NOT EXISTS quotes(
                id int NOT NULL auto_increment, 
                content text,
                tags text,
                author VARCHAR(255),
                PRIMARY KEY (id)
           )
           """)

       def process_item(self, item, spider):

          ## Define insert statement
          self.cur.execute(""" insert into quotes (content, tags, author) values (%s,%s,%s)""", (
	      item["text"],
              str(item["tags"]),
              item["author"]
          ))

          ## Execute insert of data into database
          self.conn.commit()
          return item

       def close_spider(self, spider):
     
           ## Close cursor & connection to database 
           self.cur.close()
           self.conn.close()

   - Code Pipeline Only Saving New Data with MySQL

     # pipelines.py

     import mysql.connector

     class MysqlDemoPipeline:

	
        def __init__(self):
	    ## Create/Connect to database
            self.conn = mysql.connector.connect(
               host = 'localhost',
               user = 'root',
               password = '**********',
               database = 'YOUR DATABASE NAME'
            )

            ## Create cursor, used to execute commands
            self.cur = self.conn.cursor()
        
            ## Create quotes table if none exists
            self.cur.execute("""
            CREATE TABLE IF NOT EXISTS quotes(
                id int NOT NULL auto_increment, 
                content text,
                tags text,
                author VARCHAR(255),
                PRIMARY KEY (id)
           )
           """)

 	def process_item(self, item, spider):

	    ## Check to see if text is already in database 
            self.cur.execute("select * from quotes where content = %s", (item['text'],))
            result = self.cur.fetchone()

            ## If it is in DB, create log message
            if result:
                spider.logger.warn("Item already in database: %s" % item['text'])


            ## If text isn't in the DB, insert data
            else:

                ## Define insert statement
                self.cur.execute(""" insert into quotes (content, tags, author) values (%s,%s,%s)""", (
                    item["text"],
                    str(item["tags"]),
                    item["author"]
                ))

                ## Execute insert of data into database
                self.connection.commit()
            return item

       def close_spider(self, spider):
     
           ## Close cursor & connection to database 
           self.cur.close()
           self.conn.close()

   
   References:
     - connect with postgresql: https://thepythonscrapyplaybook.com/scrapy-save-data-postgres/
     - connect with mysql: https://thepythonscrapyplaybook.com/scrapy-save-data-mysql/