{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b6c3db",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fde83fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eee1cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/movie_data.csv', encoding='utf-8')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "73dcec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384eee2",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5186dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_html_emoji(text):\n",
    "    # Remove html\n",
    "    text = re.sub('<[^>]*', '', text)\n",
    "    # Remove emoji\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def filter_text(text):\n",
    "    \n",
    "    # Tokenize and get pos words\n",
    "    pos_words = pos_tag_text(text)\n",
    "    \n",
    "    processed_words = set()\n",
    "    \n",
    "    for word, tag in pos_words:\n",
    "        if (tag == 'n') or (tag == 'J'):\n",
    "            if len(word) > 3:\n",
    "                \n",
    "                word = wnl.lemmatize(word)\n",
    "                \n",
    "                processed_words.add(word)\n",
    "    \n",
    "    return list(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2673607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brazil']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleansing\n",
    "\n",
    "def preprocessor(text):\n",
    "    # Remove html emoji\n",
    "    text = remove_html_emoji(text)\n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    # Lemmatize text\n",
    "    text_list = filter_text(text)\n",
    "    return text_list\n",
    "\n",
    "# Example\n",
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "65757d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [vote, grace, mark, carroll, screenplay, title...\n",
       "1    [watch, hour, kris, line, motion, holy, nonsen...\n",
       "2    [try, success, bruckheimer, jersey, script, ch...\n",
       "3    [watch, people, strutters, time, mirror, movie...\n",
       "4    [change, film, chorus, line, meaning, premise,...\n",
       "5    [plot, adult, history, ton, swim, ghost, year,...\n",
       "6    [liveliness, film, walk, backstage, doll, game...\n",
       "7    [line, course, tie, family, figuring, step, fr...\n",
       "8    [film, chavez, president, story, world, people...\n",
       "9    [place, film, walker, action, janine, mouse, r...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = df['review'].apply(preprocessor)\n",
    "\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd144a0",
   "metadata": {},
   "source": [
    "# Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "240bbc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 area\n",
      "1 book\n",
      "2 brazil\n",
      "3 carroll\n",
      "4 case\n",
      "5 charge\n",
      "6 christopher\n",
      "7 class\n",
      "8 connecticut\n",
      "9 cover\n",
      "10 crime\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Create dictionary\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "489c2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100_000)\n",
    "\n",
    "# NOTE:\n",
    "# - Less than 15 documents (absolute number) or\n",
    "# - more than 0.5 documents (fraction of total corpus size, not absolute number)\n",
    "# - Keep only the first 100_000 most frequent tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abb0db",
   "metadata": {},
   "source": [
    "# Generate Model with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "71bd0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 35 ('minute') appears 1\n",
      "Word 81 ('life') appears 1\n",
      "Word 123 ('video') appears 1\n",
      "Word 136 ('work') appears 1\n",
      "Word 234 ('daughter') appears 1\n",
      "Word 243 ('today') appears 1\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Preview Bag of Words\n",
    "\n",
    "bow_doc_50 = bow_corpus[50]\n",
    "\n",
    "for i in range(len(bow_doc_50)):\n",
    "    print(f\"Word {bow_doc_50[i][0]} ('{dictionary[bow_doc_50[i][0]]}') appears {bow_doc_50[i][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38f60ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"story\" + 0.023*\"time\" + 0.023*\"character\" + 0.018*\"watch\" + 0.015*\"scene\" + 0.015*\"actor\" + 0.013*\"thing\" + 0.012*\"life\" + 0.012*\"year\" + 0.011*\"world\"\n",
      "Topic: 1 \n",
      "Words: 0.020*\"horror\" + 0.018*\"idea\" + 0.018*\"plot\" + 0.015*\"year\" + 0.014*\"thing\" + 0.013*\"watch\" + 0.013*\"fan\" + 0.013*\"performance\" + 0.012*\"time\" + 0.012*\"people\"\n",
      "Topic: 2 \n",
      "Words: 0.027*\"time\" + 0.022*\"role\" + 0.018*\"people\" + 0.018*\"plot\" + 0.016*\"scene\" + 0.014*\"character\" + 0.013*\"action\" + 0.012*\"watch\" + 0.012*\"cast\" + 0.011*\"fan\"\n",
      "Topic: 3 \n",
      "Words: 0.024*\"story\" + 0.022*\"scene\" + 0.020*\"actor\" + 0.019*\"time\" + 0.016*\"director\" + 0.016*\"character\" + 0.015*\"people\" + 0.014*\"life\" + 0.012*\"year\" + 0.012*\"thing\"\n",
      "Topic: 4 \n",
      "Words: 0.024*\"time\" + 0.022*\"director\" + 0.021*\"character\" + 0.020*\"story\" + 0.018*\"scene\" + 0.014*\"actor\" + 0.013*\"comedy\" + 0.012*\"people\" + 0.012*\"line\" + 0.012*\"performance\"\n",
      "Topic: 5 \n",
      "Words: 0.029*\"time\" + 0.022*\"story\" + 0.019*\"life\" + 0.018*\"character\" + 0.016*\"people\" + 0.012*\"world\" + 0.011*\"performance\" + 0.011*\"place\" + 0.011*\"scene\" + 0.011*\"series\"\n",
      "Topic: 6 \n",
      "Words: 0.028*\"time\" + 0.021*\"people\" + 0.019*\"plot\" + 0.015*\"year\" + 0.015*\"thing\" + 0.013*\"kind\" + 0.012*\"series\" + 0.012*\"character\" + 0.012*\"love\" + 0.011*\"george\"\n",
      "Topic: 7 \n",
      "Words: 0.024*\"time\" + 0.018*\"character\" + 0.017*\"people\" + 0.016*\"story\" + 0.016*\"actor\" + 0.013*\"point\" + 0.013*\"world\" + 0.013*\"life\" + 0.013*\"performance\" + 0.012*\"thing\"\n",
      "Topic: 8 \n",
      "Words: 0.021*\"time\" + 0.015*\"scene\" + 0.015*\"thing\" + 0.013*\"year\" + 0.013*\"story\" + 0.013*\"character\" + 0.011*\"people\" + 0.011*\"actor\" + 0.011*\"wife\" + 0.010*\"fact\"\n",
      "Topic: 9 \n",
      "Words: 0.024*\"time\" + 0.022*\"character\" + 0.020*\"thing\" + 0.016*\"scene\" + 0.015*\"plot\" + 0.014*\"life\" + 0.012*\"story\" + 0.010*\"minute\" + 0.009*\"watch\" + 0.009*\"year\"\n"
     ]
    }
   ],
   "source": [
    "lda_bow = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                     num_topics=10,\n",
    "                                     id2word=dictionary,\n",
    "                                     passes=2, workers=2)\n",
    "\n",
    "# Print topic\n",
    "for idx, topic in lda_bow.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e38c3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'workout', 'purchase', 'minute', 'blessing', 'easy', 'video', 'sweat', 'weight', 'exercise', 'today', 'circulation', 'pray', 'daughter', 'born', 'routine', 'channel', 'cable', 'life']\n",
      "\n",
      "Score: 0.871400773525238\t \n",
      "Topic 0: \n",
      "0.026*\"story\" + 0.023*\"time\" + 0.023*\"character\" + 0.018*\"watch\" + 0.015*\"scene\" + 0.015*\"actor\" + 0.013*\"thing\" + 0.012*\"life\" + 0.012*\"year\" + 0.011*\"world\"\n",
      "\n",
      "Score: 0.014290498569607735\t \n",
      "Topic 3: \n",
      "0.024*\"story\" + 0.022*\"scene\" + 0.020*\"actor\" + 0.019*\"time\" + 0.016*\"director\" + 0.016*\"character\" + 0.015*\"people\" + 0.014*\"life\" + 0.012*\"year\" + 0.012*\"thing\"\n",
      "\n",
      "Score: 0.014290282502770424\t \n",
      "Topic 9: \n",
      "0.024*\"time\" + 0.022*\"character\" + 0.020*\"thing\" + 0.016*\"scene\" + 0.015*\"plot\" + 0.014*\"life\" + 0.012*\"story\" + 0.010*\"minute\" + 0.009*\"watch\" + 0.009*\"year\"\n",
      "\n",
      "Score: 0.014289828017354012\t \n",
      "Topic 5: \n",
      "0.029*\"time\" + 0.022*\"story\" + 0.019*\"life\" + 0.018*\"character\" + 0.016*\"people\" + 0.012*\"world\" + 0.011*\"performance\" + 0.011*\"place\" + 0.011*\"scene\" + 0.011*\"series\"\n",
      "\n",
      "Score: 0.014288864098489285\t \n",
      "Topic 4: \n",
      "0.024*\"time\" + 0.022*\"director\" + 0.021*\"character\" + 0.020*\"story\" + 0.018*\"scene\" + 0.014*\"actor\" + 0.013*\"comedy\" + 0.012*\"people\" + 0.012*\"line\" + 0.012*\"performance\"\n",
      "\n",
      "Score: 0.014288666658103466\t \n",
      "Topic 8: \n",
      "0.021*\"time\" + 0.015*\"scene\" + 0.015*\"thing\" + 0.013*\"year\" + 0.013*\"story\" + 0.013*\"character\" + 0.011*\"people\" + 0.011*\"actor\" + 0.011*\"wife\" + 0.010*\"fact\"\n",
      "\n",
      "Score: 0.014288656413555145\t \n",
      "Topic 7: \n",
      "0.024*\"time\" + 0.018*\"character\" + 0.017*\"people\" + 0.016*\"story\" + 0.016*\"actor\" + 0.013*\"point\" + 0.013*\"world\" + 0.013*\"life\" + 0.013*\"performance\" + 0.012*\"thing\"\n",
      "\n",
      "Score: 0.014287593774497509\t \n",
      "Topic 1: \n",
      "0.020*\"horror\" + 0.018*\"idea\" + 0.018*\"plot\" + 0.015*\"year\" + 0.014*\"thing\" + 0.013*\"watch\" + 0.013*\"fan\" + 0.013*\"performance\" + 0.012*\"time\" + 0.012*\"people\"\n",
      "\n",
      "Score: 0.014287573285400867\t \n",
      "Topic 2: \n",
      "0.027*\"time\" + 0.022*\"role\" + 0.018*\"people\" + 0.018*\"plot\" + 0.016*\"scene\" + 0.014*\"character\" + 0.013*\"action\" + 0.012*\"watch\" + 0.012*\"cast\" + 0.011*\"fan\"\n",
      "\n",
      "Score: 0.0142872529104352\t \n",
      "Topic 6: \n",
      "0.028*\"time\" + 0.021*\"people\" + 0.019*\"plot\" + 0.015*\"year\" + 0.015*\"thing\" + 0.013*\"kind\" + 0.012*\"series\" + 0.012*\"character\" + 0.012*\"love\" + 0.011*\"george\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Model\n",
    "\n",
    "print(processed_docs[50])\n",
    "print()\n",
    "for idx, score in sorted(lda_bow[bow_corpus[50]],\n",
    "                         key=lambda tup: -1 * tup[1]):\n",
    "    print(f\"Score: {score}\\t \\nTopic {idx}: \\n{lda_bow.print_topic(idx, 10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c632b",
   "metadata": {},
   "source": [
    "# Generate Model with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3c938190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.23803442652931867),\n",
      " (1, 0.15734207500678157),\n",
      " (2, 0.16006211377497004),\n",
      " (3, 0.20380103752239218),\n",
      " (4, 0.24174948604559954),\n",
      " (5, 0.19297610351571345),\n",
      " (6, 0.17980319458648258),\n",
      " (7, 0.23803442652931867),\n",
      " (8, 0.21234460410405814),\n",
      " (9, 0.1361064882546078),\n",
      " (10, 0.1491046976763981),\n",
      " (11, 0.24174948604559954),\n",
      " (12, 0.16909050652408372),\n",
      " (13, 0.1981344879510331),\n",
      " (14, 0.1913544908305697),\n",
      " (15, 0.1522596485022355),\n",
      " (16, 0.1981344879510331),\n",
      " (17, 0.17724439806243122),\n",
      " (18, 0.14258110830068366),\n",
      " (19, 0.23454466453554773),\n",
      " (20, 0.1852895767384028),\n",
      " (21, 0.06667932243151192),\n",
      " (22, 0.24174948604559954),\n",
      " (23, 0.24174948604559954),\n",
      " (24, 0.18977731152742275),\n",
      " (25, 0.21970313316476817),\n",
      " (26, 0.09410216857912154)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Preview\n",
    "for doc in tfidf_corpus:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5a453042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.010*\"character\" + 0.009*\"relationship\" + 0.009*\"scene\" + 0.008*\"watch\" + 0.008*\"plot\" + 0.008*\"story\" + 0.008*\"time\" + 0.008*\"work\" + 0.007*\"director\" + 0.007*\"people\"\n",
      "Topic: 1 \n",
      "Words: 0.011*\"scene\" + 0.010*\"character\" + 0.010*\"reason\" + 0.010*\"plot\" + 0.009*\"actor\" + 0.009*\"people\" + 0.009*\"time\" + 0.009*\"story\" + 0.008*\"shot\" + 0.008*\"woman\"\n",
      "Topic: 2 \n",
      "Words: 0.010*\"time\" + 0.009*\"story\" + 0.009*\"performance\" + 0.009*\"character\" + 0.009*\"word\" + 0.008*\"comedy\" + 0.008*\"day\" + 0.007*\"situation\" + 0.007*\"help\" + 0.007*\"scene\"\n",
      "Topic: 3 \n",
      "Words: 0.009*\"actor\" + 0.008*\"watch\" + 0.008*\"life\" + 0.008*\"character\" + 0.008*\"time\" + 0.008*\"soundtrack\" + 0.007*\"quality\" + 0.007*\"thing\" + 0.007*\"love\" + 0.007*\"people\"\n",
      "Topic: 4 \n",
      "Words: 0.011*\"year\" + 0.009*\"idea\" + 0.009*\"effect\" + 0.008*\"attempt\" + 0.008*\"fan\" + 0.008*\"time\" + 0.008*\"thing\" + 0.008*\"video\" + 0.007*\"animation\" + 0.007*\"story\"\n",
      "Topic: 5 \n",
      "Words: 0.011*\"show\" + 0.010*\"comedy\" + 0.009*\"family\" + 0.009*\"plot\" + 0.009*\"people\" + 0.008*\"question\" + 0.008*\"story\" + 0.008*\"performance\" + 0.008*\"time\" + 0.008*\"actor\"\n",
      "Topic: 6 \n",
      "Words: 0.010*\"start\" + 0.009*\"miss\" + 0.009*\"scene\" + 0.008*\"girl\" + 0.008*\"book\" + 0.008*\"break\" + 0.007*\"picture\" + 0.007*\"story\" + 0.007*\"performance\" + 0.007*\"wouldn\"\n",
      "Topic: 7 \n",
      "Words: 0.010*\"thing\" + 0.009*\"people\" + 0.009*\"horror\" + 0.007*\"watch\" + 0.007*\"actor\" + 0.007*\"script\" + 0.007*\"look\" + 0.006*\"family\" + 0.006*\"century\" + 0.006*\"opinion\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"star\" + 0.008*\"story\" + 0.008*\"life\" + 0.008*\"role\" + 0.007*\"case\" + 0.007*\"time\" + 0.007*\"series\" + 0.007*\"director\" + 0.007*\"scene\" + 0.006*\"watch\"\n",
      "Topic: 9 \n",
      "Words: 0.010*\"waste\" + 0.010*\"people\" + 0.009*\"actor\" + 0.009*\"money\" + 0.008*\"plot\" + 0.008*\"drama\" + 0.008*\"chance\" + 0.008*\"episode\" + 0.008*\"time\" + 0.008*\"problem\"\n"
     ]
    }
   ],
   "source": [
    "lda_tfidf = gensim.models.LdaMulticore(tfidf_corpus, num_topics=10,\n",
    "                                      id2word=dictionary, passes=2,\n",
    "                                      workers=4)\n",
    "\n",
    "# Print topic\n",
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f6eda0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'workout', 'purchase', 'minute', 'blessing', 'easy', 'video', 'sweat', 'weight', 'exercise', 'today', 'circulation', 'pray', 'daughter', 'born', 'routine', 'channel', 'cable', 'life']\n",
      "\n",
      "Score: 0.8713915348052979\t \n",
      "Topic 1: \n",
      "0.011*\"scene\" + 0.010*\"character\" + 0.010*\"reason\" + 0.010*\"plot\" + 0.009*\"actor\" + 0.009*\"people\" + 0.009*\"time\" + 0.009*\"story\" + 0.008*\"shot\" + 0.008*\"woman\"\n",
      "\n",
      "Score: 0.014291523024439812\t \n",
      "Topic 4: \n",
      "0.011*\"year\" + 0.009*\"idea\" + 0.009*\"effect\" + 0.008*\"attempt\" + 0.008*\"fan\" + 0.008*\"time\" + 0.008*\"thing\" + 0.008*\"video\" + 0.007*\"animation\" + 0.007*\"story\"\n",
      "\n",
      "Score: 0.01429105643182993\t \n",
      "Topic 8: \n",
      "0.008*\"star\" + 0.008*\"story\" + 0.008*\"life\" + 0.008*\"role\" + 0.007*\"case\" + 0.007*\"time\" + 0.007*\"series\" + 0.007*\"director\" + 0.007*\"scene\" + 0.006*\"watch\"\n",
      "\n",
      "Score: 0.014290460385382175\t \n",
      "Topic 2: \n",
      "0.010*\"time\" + 0.009*\"story\" + 0.009*\"performance\" + 0.009*\"character\" + 0.009*\"word\" + 0.008*\"comedy\" + 0.008*\"day\" + 0.007*\"situation\" + 0.007*\"help\" + 0.007*\"scene\"\n",
      "\n",
      "Score: 0.014290117658674717\t \n",
      "Topic 3: \n",
      "0.009*\"actor\" + 0.008*\"watch\" + 0.008*\"life\" + 0.008*\"character\" + 0.008*\"time\" + 0.008*\"soundtrack\" + 0.007*\"quality\" + 0.007*\"thing\" + 0.007*\"love\" + 0.007*\"people\"\n",
      "\n",
      "Score: 0.0142896743491292\t \n",
      "Topic 9: \n",
      "0.010*\"waste\" + 0.010*\"people\" + 0.009*\"actor\" + 0.009*\"money\" + 0.008*\"plot\" + 0.008*\"drama\" + 0.008*\"chance\" + 0.008*\"episode\" + 0.008*\"time\" + 0.008*\"problem\"\n",
      "\n",
      "Score: 0.014289439655840397\t \n",
      "Topic 0: \n",
      "0.010*\"character\" + 0.009*\"relationship\" + 0.009*\"scene\" + 0.008*\"watch\" + 0.008*\"plot\" + 0.008*\"story\" + 0.008*\"time\" + 0.008*\"work\" + 0.007*\"director\" + 0.007*\"people\"\n",
      "\n",
      "Score: 0.014289004728198051\t \n",
      "Topic 6: \n",
      "0.010*\"start\" + 0.009*\"miss\" + 0.009*\"scene\" + 0.008*\"girl\" + 0.008*\"book\" + 0.008*\"break\" + 0.007*\"picture\" + 0.007*\"story\" + 0.007*\"performance\" + 0.007*\"wouldn\"\n",
      "\n",
      "Score: 0.014288807287812233\t \n",
      "Topic 7: \n",
      "0.010*\"thing\" + 0.009*\"people\" + 0.009*\"horror\" + 0.007*\"watch\" + 0.007*\"actor\" + 0.007*\"script\" + 0.007*\"look\" + 0.006*\"family\" + 0.006*\"century\" + 0.006*\"opinion\"\n",
      "\n",
      "Score: 0.01428836490958929\t \n",
      "Topic 5: \n",
      "0.011*\"show\" + 0.010*\"comedy\" + 0.009*\"family\" + 0.009*\"plot\" + 0.009*\"people\" + 0.008*\"question\" + 0.008*\"story\" + 0.008*\"performance\" + 0.008*\"time\" + 0.008*\"actor\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Model\n",
    "\n",
    "print(processed_docs[50])\n",
    "print()\n",
    "for idx, score in sorted(lda_tfidf[bow_corpus[50]],\n",
    "                         key=lambda tup: -1 * tup[1]):\n",
    "    print(f\"Score: {score}\\t \\nTopic {idx}: \\n{lda_tfidf.print_topic(idx, 10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d6f17",
   "metadata": {},
   "source": [
    "# Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e5e6a1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.290074677331923\n",
      "\n",
      "Coherence Score:  0.3334874597832685\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_bow.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_bow = CoherenceModel(model=lda_bow, texts=processed_docs.values,\n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "coherence_bow = coherence_model_bow.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a80120cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.11301486126665\n",
      "\n",
      "Coherence Score:  0.28579792812123583\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_tfidf.log_perplexity(tfidf_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_tfidf = CoherenceModel(model=lda_tfidf, texts=processed_docs.values,\n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "coherence_tfidf = coherence_model_tfidf.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc360c9",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "68fdc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimum_lda(dictionary, corpus, texts, limit,\n",
    "                    start=2, step=1, get_result=False):\n",
    "    coherence_values = []\n",
    "    \n",
    "    for n in range(start, limit, step):\n",
    "        lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                         num_topics=n,\n",
    "                                         id2word=dictionary)\n",
    "        \n",
    "        # Create coherence\n",
    "        coherence_model = CoherenceModel(model=lda, \n",
    "                                         texts=texts,\n",
    "                                         dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "#         print(f\"Idx {n} {coherence_model.get_coherence()}\")\n",
    "    \n",
    "    \n",
    "    opt_num_topics = start + coherence_values.index(max(coherence_values))\n",
    "    \n",
    "    lda_opt = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                         num_topics=opt_num_topics,\n",
    "                                         id2word=dictionary)\n",
    "    \n",
    "    if get_result:\n",
    "        print(coherence_values)\n",
    "    \n",
    "    return lda_opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b101ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 2 0.3432437694752679\n",
      "Idx 3 0.3437149611853181\n",
      "Idx 4 0.35237707500157195\n",
      "Idx 5 0.3477098253770904\n",
      "Idx 6 0.32064933979687854\n",
      "Idx 7 0.33160943888702954\n",
      "Idx 8 0.32325571337396364\n",
      "Idx 9 0.33171903031987365\n",
      "[0.3432437694752679, 0.3437149611853181, 0.35237707500157195, 0.3477098253770904, 0.32064933979687854, 0.33160943888702954, 0.32325571337396364, 0.33171903031987365]\n",
      "Topic: 0 \n",
      "Words: 0.019*\"character\" + 0.018*\"time\" + 0.017*\"story\" + 0.015*\"plot\" + 0.013*\"life\" + 0.012*\"thing\" + 0.011*\"people\" + 0.011*\"actor\" + 0.010*\"director\" + 0.010*\"scene\"\n",
      "Topic: 1 \n",
      "Words: 0.025*\"time\" + 0.022*\"story\" + 0.018*\"scene\" + 0.017*\"character\" + 0.016*\"watch\" + 0.014*\"year\" + 0.011*\"people\" + 0.011*\"thing\" + 0.011*\"director\" + 0.011*\"actor\"\n",
      "Topic: 2 \n",
      "Words: 0.024*\"time\" + 0.019*\"character\" + 0.017*\"story\" + 0.017*\"thing\" + 0.016*\"actor\" + 0.015*\"people\" + 0.014*\"life\" + 0.010*\"place\" + 0.010*\"point\" + 0.010*\"performance\"\n",
      "Topic: 3 \n",
      "Words: 0.025*\"time\" + 0.020*\"scene\" + 0.016*\"character\" + 0.015*\"people\" + 0.013*\"story\" + 0.012*\"performance\" + 0.012*\"thing\" + 0.011*\"role\" + 0.011*\"life\" + 0.011*\"year\"\n"
     ]
    }
   ],
   "source": [
    "lda_opt_bow = get_optimum_lda(dictionary, bow_corpus,\n",
    "                              processed_docs.values,\n",
    "                              10, get_result=True)\n",
    "\n",
    "# Print topic\n",
    "for idx, topic in lda_opt_bow.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6adfd61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 2 0.30157313402188235\n",
      "Idx 3 0.321723991814657\n",
      "Idx 4 0.2912925964267198\n",
      "Idx 5 0.31993718740199706\n",
      "Idx 6 0.31452561608851454\n",
      "Idx 7 0.2973603149584484\n",
      "Idx 8 0.29502960264983324\n",
      "Idx 9 0.2939563255127682\n",
      "[0.30157313402188235, 0.321723991814657, 0.2912925964267198, 0.31993718740199706, 0.31452561608851454, 0.2973603149584484, 0.29502960264983324, 0.2939563255127682]\n",
      "Topic: 0 \n",
      "Words: 0.009*\"people\" + 0.008*\"story\" + 0.008*\"time\" + 0.008*\"life\" + 0.008*\"year\" + 0.008*\"music\" + 0.007*\"thing\" + 0.007*\"performance\" + 0.007*\"watch\" + 0.007*\"piece\"\n",
      "Topic: 1 \n",
      "Words: 0.009*\"time\" + 0.008*\"actor\" + 0.008*\"people\" + 0.008*\"director\" + 0.008*\"scene\" + 0.007*\"story\" + 0.007*\"character\" + 0.007*\"thing\" + 0.007*\"performance\" + 0.007*\"work\"\n",
      "Topic: 2 \n",
      "Words: 0.009*\"character\" + 0.008*\"plot\" + 0.008*\"actor\" + 0.008*\"story\" + 0.008*\"scene\" + 0.008*\"watch\" + 0.007*\"time\" + 0.006*\"thing\" + 0.006*\"reason\" + 0.006*\"idea\"\n"
     ]
    }
   ],
   "source": [
    "lda_opt_tfidf = get_optimum_lda(dictionary, tfidf_corpus,\n",
    "                                  processed_docs.values,\n",
    "                                  10, get_result=True)\n",
    "\n",
    "# Print topic\n",
    "for idx, topic in lda_opt_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe963b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
