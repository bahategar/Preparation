{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6f10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3b73b",
   "metadata": {},
   "source": [
    "# Implementing a Simple Majority Vote Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317c6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import operator\n",
    "\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator,\n",
    "                             ClassifierMixin):\n",
    "    \"\"\" A majority vote ensemble classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier : array-like, shape = [n_classifier]\n",
    "        Different classifiers for the ensemble\n",
    "        \n",
    "    vote : str, {'classlabel', 'probability'}\n",
    "        Default: 'classlabel'\n",
    "        If 'classlabel' the prediction is based on the argmax\n",
    "            of class labels. Else if 'probability', the argmax of the sum\n",
    "            of probabilities is used to predict the class label\n",
    "            (recommended for calibrated classifiers).\n",
    "\n",
    "    weights : array-like, shape = [n_classifiers]\n",
    "        Optional, default: None\n",
    "        If a list of 'int' or 'float' values are provided,\n",
    "            the classifiers are weighted by importance; Uses uniform\n",
    "            weights if 'weights=None'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifiers,\n",
    "                 vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for\n",
    "                                  key, value in\n",
    "                                  _name_estimators(self.classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : {array-like, sparse matrix},\n",
    "            shape = [n_examples, n_features]\n",
    "            Matrix of training examples.\n",
    "\n",
    "        y : array-like, shape = [n_examples]\n",
    "            Vector of target class labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability'\"\n",
    "                             \"or 'classlabel'; got vote(%r)\" % self.vote)\n",
    "        \n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError(\"Number of classifiers and weights\"\n",
    "                             \"must be equal; got %d weights,\"\n",
    "                             \"%d classifiers\"\n",
    "                             % (len(self.weights),\n",
    "                                len(self.classifiers)))\n",
    "        \n",
    "        # Use LabelEncoder to ensure class labels start\n",
    "        # with 0, which is important for np.argmax\n",
    "        # call in self.predict\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
    "            # cloning estimator atau model.\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        maj_vote : array-like, shape = [n_examples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.vote == \"probability\":\n",
    "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
    "        else: # 'classlabel' vote\n",
    "            \n",
    "            # Collect results from clf.predict calls\n",
    "            predictions = np.asarray([clf.predict(X)\n",
    "                                      for clf in self.classifiers_]).T\n",
    "            maj_vote = np.apply_along_axis(lambda x: np.argmax(\n",
    "                                        np.bincount(x, weights=self.weights)),\n",
    "                                           axis=1,\n",
    "                                           arr=predictions)\n",
    "\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix},\n",
    "            shape = [n_examples, n_features]\n",
    "            Training vectors, where\n",
    "            n_examples is the number of examples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        \"\"\"\n",
    "        probas = np.asarray([clf.predict_proba(X)\n",
    "                             for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0,\n",
    "                               weights=self.weights)\n",
    "\n",
    "        return avg_proba\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get classifier parameter names for GridSearch\"\"\"\n",
    "\n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_classifiers.copy()\n",
    "            for name, step in self.named_classifiers.items():\n",
    "                for key, value in step.get_params(deep=True).items():\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10e44a",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- We used the BaseEstimator and ClassifierMixin parent classes to get some base functionality for free, including the get_params and set_params methods to set and return the classifier's parameters, as well as the score method to calculate the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd542630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813da04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c10238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
